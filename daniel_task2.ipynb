{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Prepare the notebook](#toc1_)    \n",
    "  - [Import necessary libraries](#toc1_1_)    \n",
    "  - [Import the datasets](#toc1_2_)    \n",
    "- [Task 2: Data Transformation](#toc2_)    \n",
    "  - [Feature engineering and/or novel feature definition](#toc2_1_)    \n",
    "  - [Outlier detection](#toc2_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Prepare the notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Import necessary libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install outlier_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Import the datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_races = pd.read_csv('dataset/races.csv')\n",
    "df_cyclists = pd.read_csv('dataset/cyclists.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Task 2: Data Transformation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Feature engineering and/or novel feature definition](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = df_cyclists.copy()\n",
    "\n",
    "# Identify rows with missing values\n",
    "missing_before = df_imputed[df_imputed[['weight', 'height']].isnull().any(axis=1)]\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "df_imputed[['weight', 'height']] = imputer.fit_transform(df_imputed[['weight', 'height']])\n",
    "\n",
    "# Identify rows that had missing values before but are now imputed\n",
    "imputed_rows = df_imputed.loc[missing_before.index]\n",
    "\n",
    "# Display (only) the imputed rows\n",
    "print(imputed_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = df_cyclists.copy()\n",
    "\n",
    "columns_to_impute = ['weight', 'height']\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "df_imputed[columns_to_impute] = imputer.fit_transform(df_imputed[columns_to_impute])\n",
    "\n",
    "# Identify rows with missing values\n",
    "missing_before = df_cyclists[df_cyclists[columns_to_impute].isnull().any(axis=1)]\n",
    "imputed_rows = df_imputed.loc[missing_before.index]\n",
    "\n",
    "# Display the imputed rows from the new dataframe\n",
    "print(\"Imputed Rows:\\n\", imputed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe we can try KNNImputer while including the birthyear and encoding the country as a number?\n",
    "# one-hot encoding makes sense for the nationality to avoid ordinal relationships between countries.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df_imputed = df_cyclists.copy()\n",
    "\n",
    "# 1. One-Hot Encode 'nationality' to include it in the imputation process\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')  # Drop first to avoid multicollinearity\n",
    "nationality_encoded = encoder.fit_transform(df_cyclists[['nationality']])\n",
    "nationality_encoded_df = pd.DataFrame(nationality_encoded, columns=encoder.get_feature_names_out(['nationality']))\n",
    "\n",
    "pd.set_option('display.max_columns', 100) \n",
    "pd.set_option('display.max_rows', 100)\n",
    "# print(nationality_encoded_df.iloc[0])\n",
    "\n",
    "df_impute_data = pd.concat([df_cyclists[['birth_year', 'weight', 'height']], nationality_encoded_df], axis=1)\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "\n",
    "# Perform the imputation\n",
    "df_imputed_values = pd.DataFrame(imputer.fit_transform(df_impute_data), columns=df_impute_data.columns)\n",
    "\n",
    "# Replace the imputed weight and height back into the original DataFrame copy\n",
    "df_final = df_cyclists.copy()\n",
    "df_final[['weight', 'height']] = df_imputed_values[['weight', 'height']]\n",
    "\n",
    "# Identify the rows that had missing values before the imputation\n",
    "columns_to_impute = ['weight', 'height']\n",
    "missing_before = df_cyclists[df_cyclists[columns_to_impute].isnull().any(axis=1)]\n",
    "\n",
    "# Display the imputed rows with the full cyclist information\n",
    "imputed_rows = df_final.loc[missing_before.index]\n",
    "\n",
    "# Display the imputed rows (with full cyclist information)\n",
    "print(\"Imputed Rows:\\n\", imputed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclist heights\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(imputed_rows['height'].dropna(), kde=False, bins=200, color='blue')\n",
    "\n",
    "plt.title('Distribution of heights of cyclists', fontsize=16)\n",
    "plt.xlabel('Height', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_imputed = df_cyclists.copy()\n",
    "\n",
    "columns_to_impute = ['birth_year']\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "df_imputed[columns_to_impute] = imputer.fit_transform(df_imputed[columns_to_impute])\n",
    "\n",
    "# Identify rows with missing values\n",
    "missing_before = df_cyclists[df_cyclists[columns_to_impute].isnull().any(axis=1)]\n",
    "imputed_rows = df_imputed.loc[missing_before.index]\n",
    "\n",
    "# Display the imputed rows from the new dataframe\n",
    "print(\"Imputed Rows:\\n\", imputed_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Outlier detection](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iqr_outliers(df: pd.DataFrame, columns: list, columns_per_row=2):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method and plot boxplots for multiple columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        columns (list): A list of column names to check for outliers and plot boxplots.\n",
    "        columns_per_row (int): Number of columns per row for the grid layout (default: 2).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the outliers for each column.\n",
    "    \"\"\"\n",
    "    outliers = {}\n",
    "    \n",
    "    num_cols = len(columns)\n",
    "    num_rows = (num_cols + columns_per_row - 1) // columns_per_row\n",
    "    \n",
    "    # Create the grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, columns_per_row, figsize=(15, 5 * num_rows))\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        if col not in df.columns:\n",
    "            print(f\"Column {col} does not exist in the DataFrame.\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate Q1, Q3, and IQR\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers_in_col = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outliers[col] = outliers_in_col\n",
    "        \n",
    "        # Plot the data using a boxplot\n",
    "        sns.boxplot(x=df[col], ax=axes[i])\n",
    "        axes[i].set_title(f'Boxplot of {col} with IQR Outliers')\n",
    "        axes[i].set_xlabel(f'Values in {col}')\n",
    "        \n",
    "        print(f\"Number of outliers in {col}: {len(outliers_in_col)}\")\n",
    "    \n",
    "    # Remove unused subplots if the number of columns is not a perfect multiple of columns_per_row\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Specify the columns to check\n",
    "columns_to_check = ['uci_points', 'points', 'length', 'climb_total', 'average_temperature']\n",
    "\n",
    "# Detect outliers using the IQR method and plot boxplots\n",
    "outliers = plot_iqr_outliers(df_races, columns_to_check, columns_per_row=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate histograms to do a quick check for which columns might be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(df, columns_per_row=2):\n",
    "    # Select only numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    num_cols = len(numerical_cols)\n",
    "    \n",
    "    # Calculate the number of rows required based on the number of columns per row\n",
    "    num_rows = (num_cols + columns_per_row - 1) // columns_per_row\n",
    "    \n",
    "    # Set up the figure with a grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, columns_per_row, figsize=(15, 5 * num_rows))\n",
    "    \n",
    "    # Flatten the axes array in case of multiple rows\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each column\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        axes[i].hist(df[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f\"Histogram of {col}\")\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Remove any unused subplots if the number of columns is not a perfect multiple of columns_per_row\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "plot_histograms(df_cyclists)\n",
    "plot_histograms(df_races)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some Q-Q plots to determine which columns are likely normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qq_plots(df, columns_per_row=2):\n",
    "    # Select numerical columns in the dataframe\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    num_cols = len(numerical_cols)\n",
    "    \n",
    "    # Calculate the number of rows needed based on the number of columns per row\n",
    "    num_rows = (num_cols + columns_per_row - 1) // columns_per_row\n",
    "    \n",
    "    # Set up the figure with a grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, columns_per_row, figsize=(15, 5 * num_rows))\n",
    "    \n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot Q-Q plots for each numerical column\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        stats.probplot(df[col].dropna(), dist=\"norm\", plot=axes[i])\n",
    "        axes[i].set_title(f\"Q-Q Plot for {col}\")\n",
    "    \n",
    "    # Remove any unused subplots if the number of columns is not a perfect multiple of columns_per_row\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "# Generate Q-Q plots for each df\n",
    "generate_qq_plots(df_races)\n",
    "generate_qq_plots(df_cyclists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "# Apply the D'Agostino-Pearson test\n",
    "stat, p_value = normaltest(df_cyclists['weight'].dropna())\n",
    "\n",
    "if p_value > 0.05:\n",
    "    print(f\"Height: Normally distributed (p-value = {p_value:.4f})\")\n",
    "else:\n",
    "    print(f\"Height: Not normally distributed (p-value = {p_value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normally distributed variables, we can use Grubb's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Logic for test taken from Method 3 here: https://blog.finxter.com/5-best-ways-to-perform-grubbs-test-in-python/\n",
    "# This also helps to understand: https://github.com/bhattbhavesh91/outlier-detection-grubbs-test-and-generalized-esd-test-python/blob/master/grubbs-test-for-outliers.ipynb\n",
    "\n",
    "def grubbs_test(df, columns, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Grubbs' test on the specified columns of a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        columns (list): A list of column names to apply Grubbs' test.\n",
    "        alpha (float): The significance level (default: 0.05).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the results of Grubbs' test for each column.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Column {col} does not exist in the DataFrame.\")\n",
    "            continue\n",
    "        \n",
    "        data = df[col].dropna().values  # Drop any missing values\n",
    "        N = len(data)\n",
    "        \n",
    "        if N < 3:  # Ensure there's enough data to run the test\n",
    "            print(f\"Not enough data points in {col} to run Grubbs' test.\")\n",
    "            continue\n",
    "        \n",
    "        mean = np.mean(data)\n",
    "        std_dev = np.std(data, ddof=1)\n",
    "        \n",
    "        # Grubbs' test statistic\n",
    "        G_calculated = max(abs(data - mean)) / std_dev\n",
    "        \n",
    "        # Critical value for Grubbs' test\n",
    "        t_critical = t.isf(alpha / (2 * N), N - 2)\n",
    "        G_critical = ((N - 1) / np.sqrt(N)) * np.sqrt(t_critical**2 / (N - 2 + t_critical**2))\n",
    "        \n",
    "        # Check if outlier exists\n",
    "        outlier_detected = G_calculated > G_critical\n",
    "        \n",
    "        # Store results\n",
    "        results[col] = {\n",
    "            'G_calculated': G_calculated,\n",
    "            'G_critical': G_critical,\n",
    "            'outlier_detected': outlier_detected\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "columns_to_check = ['height', 'weight']\n",
    "grubbs_results = grubbs_test(df_cyclists, columns_to_check, alpha=0.05)\n",
    "\n",
    "for col, res in grubbs_results.items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  G_calculated: {res['G_calculated']:.4f}\")\n",
    "    print(f\"  G_critical: {res['G_critical']:.4f}\")\n",
    "    print(f\"  Outlier detected: {res['outlier_detected']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-normal columns, we can use Chebyshev's Inequality (better than Markov's inequality because we know the variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev_outliers(df, columns, k=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using Chebyshev's inequality on the specified columns of a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        columns (list): A list of column names to apply Chebyshev's inequality.\n",
    "        k (float): The number of standard deviations to check for outliers (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the outliers for each column.\n",
    "    \"\"\"\n",
    "    outliers = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Column {col} does not exist in the DataFrame.\")\n",
    "            continue\n",
    "        \n",
    "        data = df[col].dropna().values  # Drop any missing values\n",
    "        mean = np.mean(data)\n",
    "        std_dev = np.std(data, ddof=1)\n",
    "        \n",
    "        # Chebyshev's inequality: values greater than k standard deviations from the mean\n",
    "        lower_bound = mean - k * std_dev\n",
    "        upper_bound = mean + k * std_dev\n",
    "        outliers_in_col = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        \n",
    "        outliers[col] = {\n",
    "            'mean': mean,\n",
    "            'std_dev': std_dev,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'outliers': outliers_in_col.tolist()\n",
    "        }\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "columns_to_check = ['birth_year']\n",
    "chebyshev_results = chebyshev_outliers(df_cyclists, columns_to_check, k=3)\n",
    "\n",
    "for col, res in chebyshev_results.items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Mean: {res['mean']:.4f}\")\n",
    "    print(f\"  Std Dev: {res['std_dev']:.4f}\")\n",
    "    print(f\"  Lower Bound: {res['lower_bound']:.4f}\")\n",
    "    print(f\"  Upper Bound: {res['upper_bound']:.4f}\")\n",
    "    print(f\"  Outliers: {res['outliers']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping to multiple lines\n",
    "\n",
    "def pca_outlier_contribution_and_plot_3d(df, columns, n_components=3, threshold=2.5):\n",
    "    # Ensure only the specified columns are selected\n",
    "    df_numerical = df[columns]\n",
    "    \n",
    "    # Drop rows with missing values (NaN) in the selected columns\n",
    "    df_numerical_cleaned = df_numerical.dropna()\n",
    "    \n",
    "    # Standardize the numerical data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_numerical_cleaned)\n",
    "    \n",
    "    # Apply PCA to reduce the dimensionality of the numerical data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Reconstruct the data from the principal components\n",
    "    X_reconstructed = pca.inverse_transform(X_pca)\n",
    "    \n",
    "    # Compute the reconstruction error for each feature (column-wise error)\n",
    "    column_wise_error = (X_scaled - X_reconstructed) ** 2  # Error for each column\n",
    "    \n",
    "    # Compute the overall reconstruction error (sum across columns)\n",
    "    reconstruction_error = np.sum(column_wise_error, axis=1)\n",
    "    \n",
    "    # Flag points with high overall reconstruction error as outliers\n",
    "    outliers = reconstruction_error > threshold\n",
    "    \n",
    "    # Create a copy of the original DataFrame\n",
    "    df_copy = df.loc[df_numerical_cleaned.index].copy()  # Only keep rows without missing values\n",
    "    \n",
    "    # Add the reconstruction error and outlier flag to the copy of the DataFrame\n",
    "    df_copy['reconstruction_error'] = reconstruction_error\n",
    "    df_copy['is_outlier'] = outliers\n",
    "    \n",
    "    # Compute and display PCA loadings (weights of each feature on each principal component)\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(n_components)], index=columns)\n",
    "    \n",
    "    # Show the loadings\n",
    "    print(\"PCA Loadings (weights of each feature on the principal components):\")\n",
    "    print(loadings)\n",
    "    \n",
    "    # Create a 3D interactive plot using Plotly\n",
    "    fig = px.scatter_3d(\n",
    "        x=X_pca[:, 0],  # Principal Component 1\n",
    "        y=X_pca[:, 1],  # Principal Component 2\n",
    "        z=X_pca[:, 2],  # Principal Component 3\n",
    "        title='3D PCA Manifold with Outliers',\n",
    "        labels={'x': 'PC1', 'y': 'PC2', 'z': 'PC3'},\n",
    "        opacity=0.7,\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    fig.show()\n",
    "    \n",
    "    # If a row is an outlier, find which columns contributed most to the error\n",
    "    contribution = pd.DataFrame(column_wise_error, columns=df_numerical_cleaned.columns, index=df_numerical_cleaned.index)\n",
    "    \n",
    "    # Return DataFrame with outlier information and column-wise contributions\n",
    "    return df_copy, contribution, loadings\n",
    "\n",
    "# Example usage with your races dataset and selecting specific columns\n",
    "# Replace 'column1', 'column2', etc., with the actual column names you want to include in PCA\n",
    "selected_columns = ['points', 'uci_points', 'length', 'climb_total', 'profile', 'startlist_quality', 'average_temperature', 'cyclist_age']  # Exclude 'position' and any irrelevant columns\n",
    "outliers_df, contribution_df, loadings_df = pca_outlier_contribution_and_plot_3d(df_races, selected_columns, n_components=3, threshold=15)\n",
    "\n",
    "# Display outliers with their overall reconstruction error\n",
    "print(outliers_df[outliers_df['is_outlier']])\n",
    "\n",
    "# Display contribution of each column for the outlier rows\n",
    "outliers_contributions = contribution_df.loc[outliers_df[outliers_df['is_outlier']].index]\n",
    "print(outliers_contributions)\n",
    "\n",
    "# Show the loadings (weights of each feature on each principal component)\n",
    "print(loadings_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
