{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'algoritmo\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) è un algoritmo di clustering che identifica gruppi (cluster) di punti simili in base alla loro densità nello spazio. È particolarmente utile perché riesce a individuare cluster di forma arbitraria e a gestire il rumore (punti non appartenenti a nessun cluster). Ecco i concetti chiave su cui si basa l’algoritmo:\n",
    "\n",
    "1. Parametri principali di DBSCAN\n",
    "- Eps (epsilon): è il raggio massimo di distanza per cui due punti sono considerati vicini. Determina fino a quale distanza i punti possono essere considerati nella stessa \"densità\".\n",
    "- MinPts (Minimum Points): è il numero minimo di punti che devono trovarsi all'interno del raggio Eps affinché una regione sia considerata \"densa\".\n",
    "\n",
    "2. Tipi di punti\n",
    "- Core Point (Punto nucleo): è un punto che ha almeno MinPts punti vicini all'interno del raggio Eps.\n",
    "- Border Point (Punto di bordo): è un punto che si trova nel raggio Eps di un punto nucleo, ma che non ha abbastanza vicini per essere esso stesso un punto nucleo.\n",
    "- Noise Point (Punto di rumore): è un punto che non rientra nel raggio Eps di nessun punto nucleo, quindi viene considerato \"rumore\".\n",
    "3. Come funziona DBSCAN\n",
    "- L’algoritmo inizia selezionando un punto non visitato e verifica se ha abbastanza vicini (in base a MinPts e Eps) per essere un punto nucleo.\n",
    "- Se il punto è un punto nucleo, DBSCAN espande il cluster includendo tutti i punti raggiungibili entro il raggio Eps e ripete il processo per ogni nuovo punto nucleo trovato.\n",
    "- Se il punto non ha abbastanza vicini, viene etichettato come rumore, ma potrebbe essere successivamente ri-assegnato come punto di bordo se si trova nel raggio Eps di un altro nucleo.\n",
    "4. Vantaggi di DBSCAN\n",
    "- Identificazione di cluster di forma arbitraria: DBSCAN è in grado di individuare cluster di forme complesse, a differenza di algoritmi come K-means che tendono a formare cluster circolari.\n",
    "- Robustezza al rumore: grazie alla sua capacità di trattare i punti di rumore, DBSCAN non è influenzato negativamente dai punti anomali.\n",
    "- Non richiede specificare il numero di cluster: a differenza di K-means, DBSCAN non richiede un numero fisso di cluster a priori.\n",
    "5. Svantaggi\n",
    "- La scelta dei parametri Eps e MinPts è critica e può influire notevolmente sui risultati.\n",
    "- DBSCAN non è efficiente per dataset molto grandi in termini di memoria e tempi di calcolo, anche se può essere migliorato con strutture come KD-Tree.\n",
    "- Funziona meglio con dati in cui la densità dei cluster è uniforme; altrimenti, i parametri potrebbero non funzionare bene per identificare tutti i cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features \"supportate\"\n",
    "1. Feature Numeriche\n",
    "- Standardizzazione: È essenziale standardizzare o normalizzare le feature numeriche prima di usare DBSCAN, soprattutto se le scale delle feature sono diverse. Questo perché DBSCAN usa le distanze per definire i cluster, quindi le differenze di scala possono influenzare i risultati. La standardizzazione (usando StandardScaler di scikit-learn) porta le feature a media 0 e deviazione standard 1\n",
    "\n",
    "- Normalizzazione Min-Max: In alternativa, si può usare una normalizzazione min-max (tra 0 e 1) per restringere l’intervallo di valori. È utile quando si ha bisogno che tutte le feature siano comprese nello stesso intervallo.\n",
    "\n",
    "2. Feature Categoriali Ordinali e Nominali\n",
    "- DBSCAN funziona meglio con dati numerici continui, quindi è necessario trasformare le feature categoriche in numeriche. Ecco due approcci:\n",
    "    - One-Hot Encoding: Per feature categoriche nominali (senza ordine), usare One-Hot Encoding per rappresentarle come variabili binarie. Tuttavia, l'aggiunta di molte feature binarie potrebbe rendere difficile l'interpretazione della distanza in DBSCAN. Potresti valutare di includere solo le feature categoriche più rilevanti.\n",
    "\n",
    "    - Codifica Ordinale (Ordinal Encoding): Per le feature categoriche ordinali (con ordine), puoi assegnare valori numerici. Questo è particolarmente utile se l'ordine ha un significato (es. basso = 1, medio = 2, alto = 3).\n",
    "\n",
    "3. Feature Temporali\n",
    "- Estrazione di Feature Cicliche: Se le feature temporali sono cicliche (es. ora del giorno, giorno della settimana), rappresentale usando coordinate circolari, come seno e coseno. Questo evita distorsioni dovute alla ciclicità (es. l’ora 23 e l’ora 0 sono molto vicine)\n",
    "\n",
    "4. Feature Binari\n",
    "- Mantieni le feature binarie: Se hai feature binarie (0/1), in genere DBSCAN le gestisce bene senza necessità di ulteriori trasformazioni. Tuttavia, se le variabili binarie sono sbilanciate (es. molte feature sono 0), potrebbe essere utile riconsiderare la loro inclusione.\n",
    "\n",
    "5. Riduzione della Dimensione\n",
    "- DBSCAN soffre nei dataset ad alta dimensionalità a causa della \"maledizione della dimensionalità\". Se hai molte feature, è consigliabile ridurre la dimensionalità con tecniche come PCA o t-SNE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN in Scikit-Learn\n",
    "\n",
    "DBSCAN in scikit-learn ha diversi parametri che determinano come l’algoritmo identifica i cluster e gestisce il rumore. Ecco una panoramica completa:\n",
    "\n",
    "1. eps\n",
    "- Descrizione: eps (epsilon) rappresenta il raggio massimo di distanza tra due punti affinché possano essere considerati vicini e parte dello stesso cluster.\n",
    "- Uso: Un valore troppo piccolo di eps porta alla formazione di cluster molto piccoli e spesso considera molti punti come rumore, mentre un valore troppo grande può portare alla fusione di più cluster.\n",
    "- Default: 0.5\n",
    "- Suggerimento: Usare un grafico delle distanze K-nearest-neighbor per scegliere il valore ottimale di eps.\n",
    "2. min_samples\n",
    "- Descrizione: min_samples rappresenta il numero minimo di punti richiesti per formare un cluster. Un punto sarà considerato un punto nucleo solo se ha almeno min_samples punti vicini entro la distanza eps.\n",
    "- Uso: Un valore più alto richiede densità maggiori per formare un cluster, riducendo il rischio di cluster rumorosi.\n",
    "- Default: 5\n",
    "- Suggerimento: Un valore comune è 2 * dimensioni del dataset, ma dipende dal tipo di dati. Più alta è la dimensionalità del dataset, maggiore dovrebbe essere min_samples.\n",
    "3. metric\n",
    "- Descrizione: La metrica di distanza usata per calcolare la vicinanza tra i punti.\n",
    "- Opzioni: Include euclidean, manhattan, chebyshev, minkowski, e altre. scikit-learn permette anche di definire una funzione di distanza personalizzata.\n",
    "- Default: euclidean\n",
    "- Suggerimento: La metrica euclidean è la più comune, ma per dati con caratteristiche diverse (ad esempio, distribuzioni geografiche), altre metriche possono funzionare meglio.\n",
    "4. metric_params\n",
    "- Descrizione: Parametri aggiuntivi per la metrica di distanza, nel caso in cui una metrica richieda ulteriori parametri.\n",
    "- Uso: Può essere usato per specificare i parametri della distanza minkowski, ad esempio.\n",
    "- Default: None\n",
    "- Suggerimento: Usato raramente a meno che non sia necessaria una metrica personalizzata con parametri specifici.\n",
    "5. algorithm\n",
    "- Descrizione: Specifica l’algoritmo di ricerca utilizzato per trovare i vicini.\n",
    "Opzioni:\n",
    "- auto: Scelta automatica tra i seguenti algoritmi.\n",
    "- ball_tree: Usa una struttura dati Ball Tree per ricerche efficienti in dati ad alta dimensionalità.\n",
    "- kd_tree: Usa un KD-Tree, che è efficiente per dati di dimensione più bassa.\n",
    "- brute: Metodo di ricerca a forza bruta.\n",
    "- Default: auto\n",
    "- Suggerimento: auto è generalmente sufficiente, ma puoi scegliere manualmente per migliorare le prestazioni a seconda della dimensionalità dei dati.\n",
    "6. leaf_size\n",
    "- Descrizione: Imposta la dimensione delle foglie per gli algoritmi ball_tree e kd_tree.\n",
    "- Uso: Riducendo o aumentando leaf_size, puoi migliorare la velocità di ricerca dei vicini, in particolare per dataset grandi e strutture di alberi.\n",
    "- Default: 30\n",
    "- Suggerimento: Modificarlo solo se il tempo di esecuzione diventa un problema; valori tra 10 e 50 funzionano bene nella maggior parte dei casi.\n",
    "7. p\n",
    "- Descrizione: Il parametro p è utilizzato solo con la metrica minkowski e rappresenta l’esponente della distanza. Ad esempio, p=2 rappresenta la distanza euclidea, mentre p=1 rappresenta la distanza manhattan.\n",
    "- Default: 2\n",
    "- Suggerimento: Cambiare p solo se si sta usando minkowski e si ha un motivo specifico per preferire una metrica non euclidea.\n",
    "8. n_jobs\n",
    "- Descrizione: Specifica il numero di CPU da utilizzare per il calcolo parallelo. Se impostato su -1, usa tutte le CPU disponibili.\n",
    "- Default: None (il che significa che utilizza un solo processore)\n",
    "- Suggerimento: Se hai un dataset grande e un sistema multi-core, impostare n_jobs=-1 può migliorare notevolmente la velocità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN su dati di con densità di cluster variabile\n",
    "\n",
    "1. Clustering Gerarchico con DBSCAN\n",
    "- Una soluzione è combinare DBSCAN con il clustering gerarchico: si può prima eseguire un clustering gerarchico per identificare le zone di densità diverse e poi applicare DBSCAN su ogni cluster risultante. In questo modo, si possono identificare valori di Eps e MinPts diversi per ciascuna zona, rendendo l’algoritmo più adattabile a densità diverse.\n",
    "\n",
    "2. Riduzione delle Dimensioni (Feature Scaling)\n",
    "- La riduzione della dimensionalità, come PCA (Principal Component Analysis), può aiutare a raggruppare i punti in modo che le variazioni di densità siano meno marcate. Adattando i dati in modo che siano più omogenei, DBSCAN può lavorare meglio anche con valori globali di Eps e MinPts.\n",
    "\n",
    "3. Applicazione Iterativa di DBSCAN\n",
    "- In alcuni casi, si può applicare DBSCAN iterativamente: si parte con un valore più ampio di Eps per individuare cluster più densi e grandi, rimuovendoli poi dai dati. Successivamente, si applica DBSCAN con un valore di Eps più basso sui dati residui per rilevare i cluster di densità inferiore. Questo metodo funziona bene se i cluster hanno densità decrescenti in modo apprezzabile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trovare minpts e eps\n",
    "\n",
    "Questa tecnica per determinare Eps e MinPts in DBSCAN è basata sul grafico della distanza K-nearest-neighbor (K-Distance Plot) e segue alcuni principi per scegliere Eps più efficacemente.\n",
    "\n",
    "\n",
    "1. Scegliere un valore per MinPts e mantenere fisso MinPts\n",
    "- La prima cosa da fare è scegliere un valore per MinPts, solitamente fissato a 4 o a circa 2 volte il numero di dimensioni del dataset.\n",
    "- Una volta scelto MinPts, si userà questo valore come k (cioè il numero di vicini più vicini) per calcolare la distanza per ogni punto del dataset.\n",
    "\n",
    "2. Calcolare la distanza K-nearest-neighbor\n",
    "- Per ogni punto nel dataset, calcola la distanza al suo k-esimo vicino più vicino (dove k è uguale a MinPts).\n",
    "- Questo significa che per ogni punto nel dataset, si determina la distanza dal punto al MinPts-esimo punto più vicino.\n",
    "\n",
    "3. Ordinare le distanze e costruire un grafico\n",
    "- Ordina tutte le distanze calcolate in ordine crescente.\n",
    "- Una volta ordinati i valori di distanza, li si visualizza su un grafico dove l’asse x rappresenta i punti ordinati e l’asse y la distanza dal MinPts-esimo vicino più vicino di ciascun punto.\n",
    "\n",
    "4. Osservare il punto di gomito\n",
    "- Osserva il grafico per identificare un punto di gomito (in inglese, \"elbow point\"). Questo è il punto in cui si nota un aumento netto delle distanze.\n",
    "- Il concetto è che i punti che fanno parte di un cluster avranno il k-esimo vicino più vicino a una distanza relativamente simile, mentre i punti di rumore o outlier avranno distanze maggiori.\n",
    "- Il punto di gomito indica il valore di Eps ideale, separando i punti densi (a sinistra del gomito) dai punti più isolati (a destra del gomito).\n",
    "\n",
    "Perché funziona?\n",
    "- L'idea alla base è che i punti nei cluster avranno il k-esimo vicino a una distanza più piccola e simile rispetto ai punti di rumore.\n",
    "- Quando si imposta Eps vicino al valore della distanza al gomito, l'algoritmo riuscirà a includere i punti del cluster e, al contempo, ad escludere i punti di rumore.\n",
    "\n",
    "Questa tecnica è utile perché consente di selezionare Eps in modo sistematico anziché casuale, basandosi su un'analisi visiva della distribuzione delle distanze."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
