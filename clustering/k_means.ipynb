{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importazione librerie e caricamento dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "df_cyclists = pd.read_csv('../dataset/cyclists.csv')\n",
    "df_races = pd.read_csv('../dataset/races.csv')\n",
    "df = pd.merge(df_cyclists, df_races, left_on='_url', right_on='cyclist')\n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Come funziona l'algoritmo k-means?\n",
    "L'algoritmo segue questi passi:\n",
    "1.\tSelezione del numero di cluster (k): L'utente sceglie il numero di cluster, cio√® il valore di K, che rappresenta il numero di gruppi desiderati. Questo √® un parametro da definire in anticipo.\n",
    "2.\tInizializzazione dei centroidi: Si selezionano kkk punti casuali nel dataset (o basati su un criterio) per inizializzare i centroidi iniziali, ossia i centri di ciascun cluster.\n",
    "3.\tAssegnazione dei punti ai cluster: Ogni punto dei dati viene assegnato al cluster il cui centroide √® il pi√π vicino in base a una misura di distanza, di solito la distanza euclidea.\n",
    "4.\tAggiornamento dei centroidi: Per ciascun cluster, si ricalcola il centroide come il punto medio (media) di tutti i punti assegnati a quel cluster.\n",
    "5.\tIterazione: I passaggi 3 e 4 vengono ripetuti fino a quando i centroidi non cambiano pi√π significativamente (ossia, l'algoritmo converge) o si raggiunge un numero massimo di iterazioni.\n",
    "L'algoritmo cerca di minimizzare la somma delle distanze quadrate tra i punti di ciascun cluster e il rispettivo centroide. Questa somma rappresenta una misura di qualit√† del clustering: pi√π √® bassa, pi√π i punti sono raggruppati in modo compatto.\n",
    "\n",
    "\n",
    "Limiti di k-means\n",
    "- Specificare k in anticipo: serve conoscere il numero di cluster.\n",
    "- Sensibilit√† ai punti iniziali: la scelta dei centroidi iniziali influisce sul risultato.\n",
    "- Adatto a cluster sferici: non funziona bene con cluster di forma irregolare.\n",
    "- Sensibile agli outlier: gli outlier possono distorcere il calcolo dei centroidi.\n",
    "\n",
    "Varianti di k-means\n",
    "Una variante comune √® il k-means++, che migliora la fase di inizializzazione scegliendo i centroidi iniziali in modo pi√π strategico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means in Scikit-Learn\n",
    "L'algoritmo KMeans di Scikit-Learn ha diversi parametri configurabili che consentono di personalizzare il comportamento dell'algoritmo per adattarsi meglio ai tuoi dati e alle tue esigenze di clustering. Ecco una panoramica dei parametri principali:\n",
    "\n",
    "Parametri principali di KMeans\n",
    "1. n_clusters:\n",
    "- Numero di cluster che si desidera ottenere.\n",
    "- Valore di default: 8.\n",
    "\n",
    "2. init:\n",
    "\n",
    "- Metodo di inizializzazione dei centroidi. Pu√≤ essere:\n",
    "- 'k-means++': metodo di inizializzazione che migliora la selezione dei centroidi iniziali per accelerare la convergenza (default).\n",
    "- 'random': centroidi inizializzati casualmente.\n",
    "- Array numpy personalizzato: √® possibile fornire un array di centroidi iniziali.\n",
    "\n",
    "3. n_init:\n",
    "\n",
    "- Numero di volte in cui l'algoritmo viene eseguito con diverse inizializzazioni dei centroidi, prendendo poi il risultato con la migliore somma dei quadrati delle distanze (intra-cluster inertia).\n",
    "- Valore di default: 10.\n",
    "\n",
    "4. max_iter:\n",
    "\n",
    "- Numero massimo di iterazioni per ciascuna esecuzione dell‚Äôalgoritmo k-means.\n",
    "- Valore di default: 300.\n",
    "\n",
    "5. tol:\n",
    "\n",
    "- Tolleranza per il criterio di convergenza, cio√® il valore minimo di variazione nei centroidi al di sotto del quale l'algoritmo si considera convergente.\n",
    "- Valore di default: 1e-4.\n",
    "\n",
    "6. random_state:\n",
    "\n",
    "- Determina la riproducibilit√† dei risultati quando si usa la stessa inizializzazione.\n",
    "- Pu√≤ essere un numero intero o None.\n",
    "\n",
    "7. algorithm:\n",
    "\n",
    "- Specifica l'algoritmo da utilizzare. Pu√≤ essere:\n",
    "- 'lloyd': algoritmo standard di Lloyd per k-means.\n",
    "- 'elkan': ottimizzazione per il calcolo delle distanze che accelera l‚Äôalgoritmo, ma funziona solo se la distanza √® euclidea e per alcuni tipi di dati.\n",
    "- 'auto': seleziona automaticamente l'algoritmo pi√π efficiente in base ai dati e alla metrica scelta.\n",
    "- Valore di default: 'lloyd'.\n",
    "\n",
    "\n",
    "Altri parametri utili\n",
    "1. verbose:\n",
    "\n",
    "Livello di dettaglio dell‚Äôoutput (se maggiore di 0, mostra dettagli dell‚Äôalgoritmo durante l‚Äôesecuzione).\n",
    "\n",
    "2. copy_x:\n",
    "\n",
    "Se True, copia i dati prima di fare il clustering, altrimenti i dati originali potrebbero essere modificati.\n",
    "\n",
    "3. n_jobs:\n",
    "\n",
    "Numero di processi da utilizzare per il calcolo parallelo. (Attenzione che in Scikit-Learn recenti, il parametro n_jobs non √® pi√π supportato, e la parallelizzazione √® gestita automaticamente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "K-means si basa sulla distanza euclidea, per cui le feature devono essere numeriche e continue. Se hai feature categoriali (come colori, citt√†, ecc.), non √® consigliabile usare k-means direttamente, poich√© la distanza euclidea non √® significativa per queste variabili. In questi casi, si potrebbe:\n",
    "- Convertire le variabili categoriali in numeriche con metodi come l'embedding o il one-hot encoding (se ha senso nel contesto).\n",
    "\n",
    "\n",
    "K-means √® sensibile alla scala dei dati: feature con scale molto diverse possono dominare il calcolo delle distanze. Per evitare questo problema, √® necessario normalizzare o standardizzare i dati:\n",
    "- Normalizzazione (min-max scaling): scala le feature in un intervallo fisso, tipicamente [0,1][0, 1][0,1], per mantenere i valori tra limiti specifici.\n",
    "- Standardizzazione: trasforma le feature in modo che abbiano una media di 0 e deviazione standard di 1.\n",
    "\n",
    "Gli outlier possono influire negativamente sui centroidi perch√© distorcono la media, che √® alla base del calcolo dei centroidi in k-means. Gli outlier spesso causano la formazione di cluster \"distorti\" o disallineati rispetto alla distribuzione principale dei dati. Per questo motivo, conviene:\n",
    "- Rimuovere o mitigare gli outlier (con metodi come l'analisi delle distribuzioni, z-score, ecc.).\n",
    "\n",
    "K-means tende a identificare cluster di forma sferica (cio√® gruppi di dati che si distribuiscono intorno al centroide in modo simmetrico). Se i tuoi dati hanno cluster di forma complessa o allungata, k-means potrebbe non essere ideale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinare il numero ottimale di cluster K\n",
    "Esistono vari metodi per scegliere ùêæ, ma due dei pi√π comuni sono il metodo del gomito e il coefficiente di silhouette.\n",
    "\n",
    "- Metodo del gomito\n",
    "Esegui k-means per vari valori di K (ad esempio, da 1 a 10).\n",
    "Calcola la somma delle distanze quadrate (intra-cluster inertia) per ciascun ùêæ. Questa somma rappresenta quanto ogni punto si trova vicino al centro del cluster assegnato.\n",
    "Traccia un grafico con i valori di ùêæ sull'asse X e l'inertia sull'asse Y.\n",
    "Cerca il \"gomito\" del grafico, cio√® il punto in cui la riduzione dell'inertia inizia a diventare meno pronunciata. Questo punto rappresenta un buon compromesso tra coesione e separazione dei cluster.\n",
    "\n",
    "- Coefficiente di silhouette\n",
    "Il coefficiente di silhouette misura quanto ciascun punto √® vicino ai punti del proprio cluster rispetto a quelli di altri cluster.\n",
    "Il valore del coefficiente va da -1 a 1, dove un valore vicino a 1 indica che i punti sono ben assegnati ai cluster, mentre valori vicini a 0 o negativi indicano che i punti sono vicini a pi√π cluster o assegnati male.\n",
    "Calcola il coefficiente di silhouette per ciascun valore di K e scegli quello che massimizza il valore medio di silhouette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinare il numero di iterazioni\n",
    "Il parametro max_iter controlla il numero massimo di iterazioni per cui l'algoritmo k-means dovrebbe aggiornare i centroidi e riassegnare i punti ai cluster. Scikit-Learn utilizza un valore predefinito di 300, che in genere √® sufficiente per la maggior parte dei casi. \n",
    "Alcune considerazioni per decidere il numero di iterazioni:\n",
    "\n",
    "- Convergenza: k-means di solito converge molto prima di raggiungere il numero massimo di iterazioni. Se osservi che l‚Äôalgoritmo converge in meno iterazioni, puoi abbassare max_iter.\n",
    "- Monitoraggio: imposta un valore elevato (es. 300 o pi√π) se non sei sicuro. Se l‚Äôalgoritmo non converge entro questo limite, potresti provare a modificare l'inizializzazione dei centroidi (usando k-means++) o a cambiare il valore di K.\n",
    "- Tol: Il parametro tol rappresenta la tolleranza per considerare che i centroidi non cambiano pi√π (criterio di convergenza). Ridurre il valore di tol (ad esempio, 1e-6 invece di 1e-4) pu√≤ richiedere pi√π iterazioni, ma migliorare la precisione.\n",
    "\n",
    "In pratica, non √® sempre necessario specificare manualmente il numero di iterazioni. Di solito, la combinazione k-means++ e tol aiuta l‚Äôalgoritmo a convergere velocemente, e puoi lasciare max_iter a un valore elevato senza preoccupazioni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestire l'inizializzazione dei centroidi\n",
    "\n",
    "### Inizializzazione casuale\n",
    "\n",
    "L'inizializzazione casuale √® il metodo pi√π semplice: i centroidi iniziali vengono scelti casualmente tra i punti del dataset. Tuttavia, questa tecnica pu√≤ risultare problematica poich√©, se i centroidi sono troppo vicini o male distribuiti, l'algoritmo pu√≤ convergere a una soluzione subottimale.\n",
    "\n",
    "### Inizializzazione con k-means++\n",
    "\n",
    "L'inizializzazione k-means++ √® la tecnica standard in Scikit-Learn ed √® progettata per posizionare i centroidi iniziali in modo pi√π strategico, massimizzando la distanza tra i centroidi iniziali. Questa tecnica riduce la probabilit√† di cluster vuoti e migliora la velocit√† di convergenza, rendendo k-means pi√π efficace.\n",
    "\n",
    "Come funziona k-means++:\n",
    "\n",
    "1. Il primo centroide viene scelto casualmente tra i punti del dataset.\n",
    "2. Per ogni punto, calcola la distanza dal centroide pi√π vicino.\n",
    "3. I punti pi√π distanti dai centroidi esistenti hanno una probabilit√† maggiore di essere scelti come centroidi successivi.\n",
    "4. Ripeti il processo fino a ottenere K centroidi.\n",
    "\n",
    "### Inizializzazione basata su campionamento e clustering gerarchico\n",
    "Se k-means++ non funziona bene per dataset specifici, un altro approccio √® inizializzare i centroidi con un metodo di clustering gerarchico (ad esempio, agglomerativo) per ottenere una distribuzione preliminare dei centroidi.\n",
    "\n",
    "Procedura:\n",
    "1. Esegui un clustering gerarchico per ottenere una suddivisione preliminare dei dati.\n",
    "2. Utilizza i centroidi dei cluster gerarchici come punto di partenza per i centroidi di k-means.\n",
    "3. Esegui k-means con questi centroidi iniziali.\n",
    "\n",
    "Questo approccio pu√≤ funzionare meglio in dataset complessi, specialmente in presenza di cluster di forma e densit√† diverse.\n",
    "\n",
    "### Inizializzazione con metodi basati su distanza massima\n",
    "Un'alternativa utile √® selezionare i centroidi iniziali in modo tale che siano il pi√π distanti possibile l'uno dall'altro. Questo metodo √® simile a k-means++, ma pi√π semplice.\n",
    "\n",
    "Procedura:\n",
    "1. Scegli un punto casuale come primo centroide.\n",
    "2. Seleziona il punto pi√π distante dal centroide esistente come il prossimo centroide.\n",
    "3. Ripeti fino ad avere K centroidi.\n",
    "\n",
    "Questo metodo √® meno sofisticato di k-means++ ma pu√≤ essere efficace per evitare posizioni di centroidi troppo vicine.\n",
    "\n",
    "### Inizializzazione con metodi basati su PCA\n",
    "Per dati ad alta dimensionalit√†, puoi ridurre la dimensionalit√† con PCA e usare le coordinate dei componenti principali per ottenere i centroidi iniziali.\n",
    "\n",
    "Procedura:\n",
    "1. Applica PCA sul dataset e riducilo a un numero inferiore di dimensioni (ad esempio, 2 o 3).\n",
    "2. Esegui k-means sui dati ridotti e usa i centroidi trovati come centroidi iniziali per k-means sull‚Äôintero dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sulla natura dei dati\n",
    "\n",
    "Le caratteristiche intrinseche dei dati possono influenzare significativamente i risultati del clustering e la scelta dell‚Äôalgoritmo.\n",
    "\n",
    "1. Misura di prossimit√† o densit√†:\n",
    "\n",
    "- Le misure di distanza (come la distanza euclidea) sono fondamentali per molti algoritmi di clustering, come k-means. Tuttavia, il tipo di metrica usata dovrebbe adattarsi alle caratteristiche dei dati e all'applicazione. Per dati con relazioni particolari (come dati spaziali o temporali), una misura di prossimit√† standard potrebbe non catturare correttamente la struttura dei cluster.\n",
    "\n",
    "2. Caratteristiche dei dati che influenzano la prossimit√† e/o la densit√†:\n",
    "\n",
    "- Dimensionalit√†: Nei dati ad alta dimensionalit√†, molte metriche di distanza diventano meno informative, perch√© i punti tendono a diventare equidistanti tra loro. Inoltre, la presenza di molte dimensioni rende i dati pi√π sparsi, riducendo l‚Äôefficacia di algoritmi come k-means.\n",
    "- Tipo di attributo: Se i dati includono attributi categoriali o ordinali, le metriche di distanza come quella euclidea non sono adatte, richiedendo altre misure o adattamenti.\n",
    "- Relazioni speciali: Se i dati sono correlati (ad esempio, con autocorrelazione in dati temporali o spaziali), la prossimit√† calcolata in modo tradizionale potrebbe non rappresentare la vera struttura del cluster.\n",
    "\n",
    "3. Distribuzione dei dati:\n",
    "\n",
    "- Se i dati non sono distribuiti in modo simile (ad esempio, con densit√† diverse o forme di cluster non sferiche), algoritmi come k-means potrebbero non funzionare bene. Alcuni algoritmi (come DBSCAN) sono pi√π adatti a dati con distribuzioni irregolari.\n",
    "\n",
    "4. Rumore e outlier:\n",
    "\n",
    "- Outlier o rumore possono distorcere i risultati di molti algoritmi di clustering, in quanto influenzano i centroidi o le medie dei cluster.\n",
    "\n",
    "\n",
    "Impatto della correlazione sui dati e il clustering\n",
    "\n",
    "- Se i dati sono correlati, ci sono diverse sfide:\n",
    "\n",
    "Metriche di distanza inappropriate: Nei dati correlati, due variabili possono rappresentare informazioni simili, e la loro combinazione potrebbe dare pi√π peso a un'informazione rispetto ad altre. Questo pu√≤ confondere la misura di distanza e portare a cluster meno precisi.\n",
    "- Autocorrelazione: Nei dati temporali o spaziali, i punti vicini tendono a essere pi√π simili tra loro rispetto ai punti lontani. Usare una semplice misura di distanza potrebbe quindi non catturare questa struttura e portare a cluster che non riflettono le relazioni spaziali o temporali reali.\n",
    "\n",
    "Come affrontare la correlazione nei dati di clustering\n",
    "\n",
    "Per dati con forte correlazione, ecco alcune strategie utili:\n",
    "\n",
    "- Riduzione della dimensionalit√†: Tecniche come PCA (Principal Component Analysis) possono aiutare a ridurre la correlazione tra le variabili prima del clustering.\n",
    "- Misure di distanza personalizzate: In alcuni casi, √® possibile definire una misura di distanza che tenga conto della correlazione, come una distanza basata sulla correlazione stessa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capire in anticipo se i cluster hanno forma sferica\n",
    "\n",
    "1. Visualizzazione dei dati (se possibile)\n",
    "Se i dati hanno due o tre dimensioni, puoi visualizzarli direttamente e osservare la loro struttura:\n",
    "\n",
    "- Scatter plot: Per dati bidimensionali, un grafico scatter pu√≤ mostrare se i cluster hanno forme allungate, curve o non sferiche.\n",
    "- Riduzione della dimensionalit√†: Per dati ad alta dimensionalit√†, puoi usare tecniche di riduzione della dimensionalit√† come PCA o t-SNE per proiettare i dati in due o tre dimensioni e vedere se i cluster hanno forme irregolari -> Se i cluster sembrano avere forme allungate, curve o irregolari nella proiezione bidimensionale, √® un buon indicatore che i cluster sono non sferici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perch√© la varianza delle feature √® importante in k-means?\n",
    "\n",
    "1. Squilibrio tra feature:\n",
    "\n",
    "Se una feature ha una varianza molto pi√π alta delle altre, dominer√† il calcolo della distanza euclidea. Questo pu√≤ causare una distorsione nei cluster, poich√© k-means cercher√† di minimizzare le distanze in base a questa feature dominante, ignorando o minimizzando l'importanza delle altre.\n",
    "\n",
    "2. Differenti scale:\n",
    "\n",
    "La varianza elevata di una feature pu√≤ essere dovuta a scale diverse tra le feature (ad esempio, una feature misurata in migliaia e un‚Äôaltra in unit√†). K-means tende a dare maggiore peso alle feature con valori pi√π grandi, causando cluster sbilanciati se le feature non sono sulla stessa scala.\n",
    "\n",
    "3. PCA e riduzione della dimensionalit√†:\n",
    "\n",
    "Se utilizzi PCA prima di k-means, la varianza viene ridistribuita nei componenti principali. √à importante considerare quanta varianza mantenere (ad esempio, 90-95%) per assicurarti che i componenti principali contengano abbastanza informazioni utili per il clustering.\n",
    "\n",
    "Come gestire la varianza nelle feature per k-means?\n",
    "1. Standardizzazione delle feature\n",
    "√à buona pratica standardizzare o normalizzare le feature per ridurre l‚Äôimpatto della varianza non bilanciata. Standardizzare significa trasformare ogni feature in modo che abbia media 0 e deviazione standard 1, rendendo tutte le feature comparabili. Questo evita che una feature con varianza alta domini il clustering.\n",
    "\n",
    "2. Monitoraggio della varianza nella PCA\n",
    "Se usi PCA per ridurre la dimensionalit√† prima di k-means, puoi monitorare la quantit√† di varianza spiegata da ciascun componente principale e scegliere il numero di componenti in modo che conservino una sufficiente quantit√† di informazione (tipicamente il 90-95% della varianza)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitazioni di k-means su cluster di dimensioni o separazioni diverse\n",
    "1. Differenze di dimensioni dei cluster: k-means assume che i cluster siano di dimensioni simili e sferici. Quando i cluster hanno dimensioni diverse, l'algoritmo tende a sbilanciarsi, poich√© cerca di assegnare i punti al centroide pi√π vicino senza tenere conto della diversa distribuzione dei dati tra i cluster.\n",
    "\n",
    "2. Cluster non ben separati: Quando i cluster sono vicini o non ben separati, k-means pu√≤ avere difficolt√† a distinguerli. I cluster ben separati hanno generalmente una distanza elevata tra i centroidi o una distanza minima elevata tra i punti di ciascun cluster. Se questa distanza non √® elevata, k-means pu√≤ sovrapporre i cluster o assegnare punti al cluster sbagliato.\n",
    "\n",
    "### Superare queste limitazioni con cluster pi√π numerosi\n",
    "Un modo per superare queste limitazioni √® utilizzare un numero maggiore di cluster in k-means, quindi eseguire un'analisi successiva (post-processing) per unire i cluster pi√π vicini. Questo approccio pu√≤ aiutare a:\n",
    "\n",
    "- Rilevare sotto-cluster all'interno di cluster pi√π grandi o complessi, rappresentando meglio la struttura dei dati.\n",
    "- Unire cluster vicini in un secondo momento, ad esempio valutando la distanza tra i centroidi o utilizzando una misura di contiguit√† (distanza minima tra i punti appartenenti a cluster diversi) per identificare quali cluster potrebbero essere considerati parte dello stesso gruppo.\n",
    "\n",
    "### Procedura di post-processing\n",
    "1. Dividere inizialmente in molti cluster: Avvia k-means con un numero elevato di cluster per rilevare sotto-strutture.\n",
    "\n",
    "2. Calcolare le distanze tra i cluster: Dopo aver ottenuto i centroidi dei sotto-cluster, calcola le distanze tra di essi:\n",
    "\n",
    "- Distanza tra i centroidi: Se due cluster hanno centroidi molto vicini, potrebbero essere parte dello stesso gruppo.\n",
    "- Distanza tra punti pi√π vicini (contiguit√†): Se la distanza minima tra i punti di due cluster √® molto bassa, puoi considerare di unirli.\n",
    "\n",
    "3. Unione dei cluster vicini: Utilizza una soglia di distanza per unire i cluster contigui e formare i cluster finali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perch√© k-means fatica con cluster di densit√† diversa\n",
    "1. istanza euclidea: k-means si basa sulla distanza euclidea, quindi cerca di trovare cluster di forma sferica e con densit√† simile. Cluster pi√π densi tenderanno ad attrarre pi√π punti vicini e potranno inglobare cluster meno densi vicini.\n",
    "\n",
    "2. Distribuzione dei punti: Nei cluster a densit√† variabile, la distanza dal centroide non riflette accuratamente la \"vicinanza\" all'interno di un cluster. I punti dei cluster a bassa densit√† sono pi√π distanti, e ci√≤ pu√≤ portare a una maggiore sovrapposizione tra cluster.\n",
    "\n",
    "### Strategia per gestire cluster di densit√† diversa con k-means\n",
    "1. Applicare k-means con un numero elevato di cluster:\n",
    "\n",
    "Avvia k-means con un numero di cluster maggiore di quello atteso (ad esempio, 2-3 volte il numero di cluster che ti aspetti).\n",
    "L'obiettivo √® ottenere cluster pi√π piccoli e compatti che rappresentino sotto-aree dei cluster principali, permettendo di catturare aree di diversa densit√† all'interno dello stesso cluster.\n",
    "\n",
    "2. Misurare la vicinanza tra i cluster:\n",
    "\n",
    "Dopo aver ottenuto i cluster, calcola la distanza tra i centroidi dei cluster oppure usa una misura di contiguit√† (ad esempio, la distanza tra i punti pi√π vicini di due cluster).\n",
    "Identifica i cluster vicini tra loro e che potrebbero appartenere allo stesso gruppo principale.\n",
    "\n",
    "3. Unire i cluster vicini in gruppi finali:\n",
    "\n",
    "Usa una soglia di distanza per decidere quali cluster unire. Puoi basarti su:\n",
    "\n",
    "- Distanza tra centroidi: Se due centroidi sono molto vicini, probabilmente appartengono allo stesso cluster principale.\n",
    "- Distanza minima tra punti (contiguit√†): Se due cluster hanno una bassa distanza tra i loro punti pi√π vicini, possono essere considerati parte dello stesso gruppo.\n",
    "- Dopo aver identificato i cluster vicini, uniscili in gruppi finali per rappresentare i cluster principali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Come si comporta k-means con cluster vuoti\n",
    "1. Ricalcolo dei centroidi:\n",
    "\n",
    "Se un cluster rimane vuoto in un'iterazione, k-means in Scikit-Learn (e in molte altre implementazioni) riassegna il centroide di quel cluster a una posizione diversa, basandosi su uno dei due metodi pi√π comuni:\n",
    "\n",
    "- Riposizionamento casuale: Il centroide vuoto viene assegnato a una posizione casuale all'interno del dataset.\n",
    "- Riposizionamento al punto pi√π distante: In alcune implementazioni, il centroide vuoto viene assegnato al punto pi√π distante da tutti i centroidi esistenti, cercando di massimizzare la separazione dei cluster.\n",
    "\n",
    "2. Continuazione del clustering:\n",
    "\n",
    "Dopo il riposizionamento, l'algoritmo continua normalmente, riassegnando i punti ai centroidi e aggiornando i centroidi finch√© non si raggiunge la convergenza o il numero massimo di iterazioni.\n",
    "I cluster vuoti possono essere problematici se si verificano in pi√π iterazioni, segnalando che il numero di cluster scelto potrebbe essere troppo elevato per la struttura dei dati.\n",
    "\n",
    "### Come gestire i cluster vuoti in k-means\n",
    "Se noti che i cluster vuoti si verificano spesso, ci sono alcune azioni che puoi intraprendere:\n",
    "\n",
    "1. Riduci il numero di cluster:\n",
    "\n",
    "I cluster vuoti spesso indicano che il numero di cluster specificato √® troppo alto rispetto alla struttura effettiva dei dati. Ridurre il valore di K potrebbe aiutare a evitare che alcuni cluster rimangano vuoti.\n",
    "\n",
    "2. Controlla la distribuzione dei dati:\n",
    "\n",
    "Se i dati hanno densit√† irregolari o aree vuote, potrebbe essere difficile per k-means formare cluster significativi in queste regioni. Considera di ridurre la dimensionalit√† o di eseguire un‚Äôanalisi esplorativa per capire meglio la struttura dei dati.\n",
    "\n",
    "3. Usa un metodo alternativo di inizializzazione dei centroidi:\n",
    "\n",
    "Il metodo di inizializzazione k-means++ √® una scelta migliore rispetto all'inizializzazione casuale, poich√© cerca di posizionare i centroidi in modo pi√π strategico e a distanze inizialmente ottimali. Questo riduce la probabilit√† di cluster vuoti e migliora la qualit√† della convergenza.\n",
    "\n",
    "4. Post-processing:\n",
    "\n",
    "Se hai bisogno di mantenere un certo numero di cluster, puoi applicare una logica di post-processing per rilevare e gestire i cluster vuoti, come il riposizionamento forzato dei centroidi nei punti pi√π densi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster validity\n",
    "\n",
    "La Cluster Validity si valuta dopo aver eseguito l‚Äôalgoritmo per misurare la qualit√† dei cluster finali e ottimizzare i parametri del modello, come K e max_iter. Ripetere l‚Äôalgoritmo con configurazioni diverse e valutare la validit√† ti permette di scegliere la configurazione ottimale.\n",
    "\n",
    "### Misure Interne\n",
    "- WSS (Within-Cluster Sum of Squares): Somma delle distanze quadrate dei punti dai centroidi, quindi la coesione interna. Applicabile su dati numerici su cui √® possibile calcolare distanze euclidee o altre metriche basate su centri dei cluster.\n",
    "- BSS (Between-Cluster Sum of Squares): Distanza tra i centroidi e il centroide globale, pesata per la dimensione del cluster,  valutando la separazione. Applicabile a dati numerici dove sia possibile calcolare i centroidi e le distanze tra di essi.\n",
    "- Rapporto BSS/WSS: Valori pi√π alti indicano una buona separazione e coesione.\n",
    "- Silhouette Score: Calcola quanto un punto √® vicino ai punti del proprio cluster rispetto a quelli di altri cluster. La silhouette varia tra -1 e 1, dove valori pi√π alti indicano una buona coesione e separazione. Applicabile a dati numerici o categorici codificati numericamente, purch√© sia possibile calcolare una distanza tra i punti.\n",
    "\n",
    "\n",
    "### Misure esterne\n",
    "- Via correlazione: La correlazione pu√≤ essere utilizzata come misura esterna per valutare la somiglianza tra le assegnazioni di cluster e le etichette delle classi, specialmente quando i dati sono ordinali o numerici. L'idea √® misurare quanto le etichette di cluster seguano l‚Äôandamento delle classi di riferimento.\n",
    "\n",
    "1. Pearson: nel contesto del clustering, viene usata per quantificare la somiglianza lineare tra le etichette dei cluster e le classi. Dati numerici o ordinali. Le etichette devono avere un significato numerico (anche ordinali trasformate in numeriche) per ottenere risultati interpretabili. Valori vicini a +1 indicano una forte correlazione positiva lineare tra etichette di cluster e classi; valori vicini a 0 indicano poca o nessuna correlazione lineare.\n",
    "\n",
    "2. Spearman:  Converte i valori in ranghi e calcola la correlazione tra questi. Dati ordinali o numerici su cui ha senso il concetto di rango. Valori vicini a +1 indicano una correlazione monotona positiva, mentre valori vicini a -1 indicano una correlazione monotona negativa. √à utile per etichette di cluster e classi che seguono relazioni non lineari ma monotone. Spearman √® preferibile a Pearson quando non √® garantita una relazione lineare tra le etichette dei cluster e le classi.\n",
    "\n",
    "3. Kendall Tau: valuta la concordanza nelle posizioni relative di coppie di punti nelle due variabili, misurando se l‚Äôordine relativo tra due variabili √® mantenuto. √à molto robusta per dati con pochi valori distinti o ordinali. Dati ordinali, ideali per etichette numeriche con pochi valori distinti o quando si vuole minimizzare l‚Äôimpatto di variazioni casuali. Valori vicini a +1 indicano forte concordanza nell‚Äôordine delle etichette di cluster e delle classi, valori vicini a -1 indicano discordanza. Adatta per etichette ordinali o numeriche che presentano un ordine relativo tra classi e cluster.\n",
    "\n",
    "    Raccomandazioni:\n",
    "\n",
    "    Pearson √® adatta solo quando esiste una relazione lineare tra etichette dei cluster e classi.\n",
    "    Spearman √® pi√π flessibile e pu√≤ gestire relazioni monotone non lineari, adatta per etichette numeriche o ordinali.\n",
    "    Kendall Tau √® preferibile per dati ordinali o con pochi valori distinti, misurando la concordanza nelle posizioni relative.\n",
    "    Queste misure di correlazione ti permettono di valutare se i cluster seguono la struttura delle classi reali, offrendo una prospettiva utile sulla qualit√† del clustering rispetto alle etichette di riferimento.\n",
    "\n",
    "\n",
    "- Entropy o purity: sono utilizzate per valutare quanto ciascun cluster sia \"puro\" rispetto alle classi di riferimento o quanto le classi siano distribuite nei cluster. Queste misure sono particolarmente utili per verificare se i cluster generati rispecchiano effettivamente le classi, misurando la coesione e la separazione tra cluster e classi.\n",
    "\n",
    "1. Purit√†:  quanto ogni cluster sia composto da punti di una singola classe. Viene calcolata selezionando, per ogni cluster, la classe pi√π rappresentata e sommando il numero di punti appartenenti a questa classe per tutti i cluster. Dati categoriali (etichette di riferimento e assegnazioni di cluster). Valori vicini a 1 indicano una forte corrispondenza tra i cluster e le classi, cio√® i cluster sono composti principalmente da punti di una singola classe. Valori pi√π bassi indicano che i cluster sono pi√π \"misti\", contenendo punti di pi√π classi.\n",
    "\n",
    "2. Entropia: misura quanto le classi siano distribuite all'interno di ciascun cluster. Valori pi√π bassi indicano cluster \"puri\" in cui una singola classe predomina, mentre valori pi√π alti indicano che il cluster contiene punti di pi√π classi. Dati categoriali (etichette di riferimento e assegnazioni di cluster). ntropia bassa indica cluster omogenei, ciascuno dominato da una sola classe, mentre un‚Äôentropia pi√π alta indica cluster eterogenei.\n",
    "\n",
    "    Confronto tra Purit√† ed Entropia\n",
    "    - Purit√†: Misura quanto i cluster siano puri, ossia composti principalmente da punti di una singola classe. Pi√π la purit√† √® alta, meglio i cluster rappresentano classi uniche. Tuttavia, √® una misura pi√π semplice e potrebbe non essere sufficientemente sensibile per indicare mescolanze di classi minori.\n",
    "    - Entropia: Misura la distribuzione delle classi all'interno dei cluster, quindi √® utile per identificare cluster eterogenei. L‚Äôentropia √® una misura pi√π dettagliata rispetto alla purit√†, ma potrebbe essere pi√π difficile da interpretare in alcuni casi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
