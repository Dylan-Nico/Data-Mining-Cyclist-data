{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Prepare the notebook](#toc1_)    \n",
    "  - [Import necessary libraries](#toc1_1_)    \n",
    "  - [Import the datasets](#toc1_2_)    \n",
    "- [PCA](#toc2_)    \n",
    "- [Distributional approach](#toc3_)    \n",
    "- [Connectivity approach](#toc4_)    \n",
    "- [One-class SVM](#toc5_)    \n",
    "- [Isolation forest](#toc6_)    \n",
    "  - [Get the final list of 'outlier' columns get getting the columns that were identified by a majority of tests](#toc6_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Prepare the notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Import necessary libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install outlier_utils\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Import the datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the dataset with the already imputed values. This prevents us from having to ignore a ton of rows (since the outlier detection tests cannot run if there are missing values, we would need to drop the rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_races = pd.read_csv('../dataset/df_races_no_missing.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100) \n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[PCA](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping to multiple lines\n",
    "\n",
    "def pca_outlier_contribution_and_plot_3d(df, columns, n_components=3, threshold=2.5):\n",
    "    \"\"\"\n",
    "    Perform PCA-based outlier detection and return a DataFrame with outlier information.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        columns (list): A list of column names to apply PCA outlier detection.\n",
    "        n_components (int): Number of principal components to use (default: 3).\n",
    "        threshold (float): The reconstruction error threshold for identifying outliers (default: 2.5).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with reconstruction error and outlier flag.\n",
    "    \"\"\"\n",
    "    # Ensure only the specified columns are selected\n",
    "    df_numerical = df[columns]\n",
    "    \n",
    "    # Drop rows with missing values (NaN) in the selected columns\n",
    "    df_numerical_cleaned = df_numerical.dropna()\n",
    "    \n",
    "    # Standardize the numerical data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_numerical_cleaned)\n",
    "    \n",
    "    # Apply PCA to reduce the dimensionality of the numerical data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Reconstruct the data from the principal components\n",
    "    X_reconstructed = pca.inverse_transform(X_pca)\n",
    "    \n",
    "    # Compute the reconstruction error for each feature (column-wise error)\n",
    "    column_wise_error = (X_scaled - X_reconstructed) ** 2  # Error for each column\n",
    "    \n",
    "    # Compute the overall reconstruction error (sum across columns)\n",
    "    reconstruction_error = np.sum(column_wise_error, axis=1)\n",
    "    \n",
    "    # Flag points with high overall reconstruction error as outliers\n",
    "    outliers = reconstruction_error > threshold\n",
    "    \n",
    "    # Create a copy of the original DataFrame to store outlier information\n",
    "    df_copy = df.loc[df_numerical_cleaned.index].copy()  # Only keep rows without missing values\n",
    "    \n",
    "    # Add the reconstruction error and outlier flag to the copy of the DataFrame\n",
    "    df_copy['reconstruction_error'] = reconstruction_error\n",
    "    df_copy['pca_outlier'] = outliers\n",
    "    \n",
    "    # Compute and display PCA loadings (weights of each feature on each principal component)\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(n_components)], index=columns)\n",
    "    \n",
    "    # Show the loadings\n",
    "    print(\"PCA Loadings (weights of each feature on the principal components):\")\n",
    "    print(loadings)\n",
    "    \n",
    "    # Create a 3D interactive plot using Plotly\n",
    "    fig = px.scatter_3d(\n",
    "        x=X_pca[:, 0],  # Principal Component 1\n",
    "        y=X_pca[:, 1],  # Principal Component 2\n",
    "        z=X_pca[:, 2],  # Principal Component 3\n",
    "        title='3D PCA Manifold with Outliers',\n",
    "        labels={'x': 'PC1', 'y': 'PC2', 'z': 'PC3'},\n",
    "        opacity=0.7,\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    fig.show()\n",
    "    \n",
    "    # If a row is an outlier, find which columns contributed most to the error\n",
    "    contribution = pd.DataFrame(column_wise_error, columns=df_numerical_cleaned.columns, index=df_numerical_cleaned.index)\n",
    "    \n",
    "    # Return DataFrame with outlier information and column-wise contributions\n",
    "    return df_copy, contribution, loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns = ['points', 'uci_points', 'length', 'climb_total', 'profile', 'startlist_quality', 'average_temperature', 'cyclist_age']  # Exclude 'position' and any irrelevant columns\n",
    "selected_columns = ['points', 'uci_points', 'length', 'climb_total', 'profile', 'startlist_quality', 'cyclist_age']  # Exclude 'position' and any irrelevant columns\n",
    "df_with_outliers, contribution_df, loadings_df = pca_outlier_contribution_and_plot_3d(df_races, selected_columns, n_components=3, threshold=15)\n",
    "\n",
    "# Display outliers with their overall reconstruction error\n",
    "print(df_with_outliers[df_with_outliers['pca_outlier']])\n",
    "\n",
    "# Display contribution of each column for the outlier rows\n",
    "outliers_contributions = contribution_df.loc[df_with_outliers[df_with_outliers['pca_outlier']].index]\n",
    "print(outliers_contributions)\n",
    "\n",
    "# Show the loadings (weights of each feature on each principal component)\n",
    "print(loadings_df)\n",
    "\n",
    "# Drop the reconstruction_error column since we don't want it from now on\n",
    "df_with_outliers.drop('reconstruction_error', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Distributional approach](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_test(df):\n",
    "    # Select only numeric columns from df\n",
    "    numeric_columns = df.select_dtypes(include=\"number\")\n",
    "    numeric_data = numeric_columns.dropna()  # Drop rows with missing numeric values. Consider imputation if needed.\n",
    "\n",
    "    # Fit the Gaussian Mixture Model to the numeric data\n",
    "    k = 5\n",
    "    algorithm = GaussianMixture(n_components=k, random_state=42)\n",
    "    algorithm.fit(numeric_data.values)\n",
    "\n",
    "    # Predict probabilities and compute outlier scores\n",
    "    outlier_degrees_per_normal = algorithm.predict_proba(numeric_data.values)\n",
    "    outlier_degrees = outlier_degrees_per_normal.max(axis=1)  # Higher values indicate more normal, lower values more likely outlier\n",
    "\n",
    "    # Create DataFrame of outlier degrees\n",
    "    outlier_scores_df = pd.DataFrame(outlier_degrees, index=numeric_data.index, columns=[\"density_degree\"])\n",
    "\n",
    "    # Define threshold (e.g., below the 5th percentile could be outliers)\n",
    "    threshold = outlier_scores_df[\"density_degree\"].quantile(0.05)\n",
    "    outlier_series = pd.Series(False, index=df.index)\n",
    "    outlier_series[numeric_data.index] = outlier_scores_df[\"density_degree\"] < threshold\n",
    "\n",
    "    # Merge the outlier degree and boolean outlier column back into the original DataFrame\n",
    "    df[\"density_degree\"] = outlier_scores_df[\"density_degree\"]\n",
    "    df[\"gmm_outlier\"] = outlier_series\n",
    "\n",
    "    # Sort the DataFrame by \"density_degree\" in ascending order for review\n",
    "    df_sorted = df.sort_values(by=\"density_degree\", ascending=True)\n",
    "\n",
    "    # Plot sorted outlier degrees\n",
    "    sorted_outlier_degrees = sorted(outlier_degrees)\n",
    "    sns.lineplot(\n",
    "        x=range(len(sorted_outlier_degrees)),\n",
    "        y=sorted_outlier_degrees\n",
    "    )\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_outliers = gmm_test(df_with_outliers)\n",
    "\n",
    "outlier_rows = df_with_outliers[df_with_outliers[\"gmm_outlier\"] == True]\n",
    "print(outlier_rows)\n",
    "\n",
    "# Drop the density degree column since we don't want it from now on\n",
    "df_with_outliers.drop('density_degree', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Connectivity approach](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectivity_approach(df):\n",
    "    import seaborn as sns\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "    # Select only numeric columns from df\n",
    "    numeric_columns = df.select_dtypes(include=\"number\")\n",
    "    numeric_data = numeric_columns.dropna()  # Drop rows with missing numeric values. Consider imputation if needed.\n",
    "\n",
    "    # Initialize the Local Outlier Factor model\n",
    "    k = 25\n",
    "    algorithm = LocalOutlierFactor(n_neighbors=k)\n",
    "\n",
    "    # Fit the model and predict outliers only on the rows without missing values\n",
    "    outlier_predictions = algorithm.fit_predict(numeric_data)\n",
    "\n",
    "    # Create a Series with the same index as the original DataFrame, defaulting to False\n",
    "    outlier_series = pd.Series(False, index=df.index)\n",
    "\n",
    "    # Assign True where the rows are outliers\n",
    "    outlier_series[numeric_data.index] = (outlier_predictions == -1)\n",
    "\n",
    "    # Add the \"connectivity_approach\" column to the original DataFrame\n",
    "    df[\"connectivity_approach_outlier\"] = outlier_series\n",
    "\n",
    "    # Negated outlier scores: the higher, the more of an outlier\n",
    "    outlier_degrees = -algorithm.negative_outlier_factor_\n",
    "    sorted_outlier_degrees = sorted(outlier_degrees)\n",
    "\n",
    "    # to achieve comparable degrees among different algorithms,\n",
    "    # we normalize distance-based scores by the maximum possible distance\n",
    "    # distances = squareform(pdist(data_only_dataset.values))\n",
    "    # maximum_possible_radius = distances.max()\n",
    "    # normalization_factor = maximum_possible_radius / k\n",
    "    # normalized_sorted_outlier_degrees = sorted_outlier_degrees / normalization_factor\n",
    "    # dataset_with_outlier_scores.loc[:, \"lof_degree_normalized\"] = outlier_degrees / normalization_factor\n",
    "\n",
    "    # Plot the sorted outlier degrees\n",
    "    sns.lineplot(\n",
    "        x=range(len(sorted_outlier_degrees)),\n",
    "        y=sorted_outlier_degrees\n",
    "    )\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_outliers = connectivity_approach(df_with_outliers)\n",
    "\n",
    "outlier_rows = df_with_outliers[df_with_outliers[\"connectivity_approach_outlier\"] == True]\n",
    "print(outlier_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[One-class SVM](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_class_svm(df):\n",
    "    from sklearn.svm import OneClassSVM\n",
    "\n",
    "    # Select only numeric columns from df\n",
    "    numeric_columns = df.select_dtypes(include=\"number\")\n",
    "    numeric_data = numeric_columns.dropna()  # Drop rows with missing numeric values. Consider imputation if needed.\n",
    "\n",
    "    # Initialize the OneClassSVM model\n",
    "    kernel = \"linear\"\n",
    "    algorithm = OneClassSVM(kernel=kernel, nu=0.9)\n",
    "    algorithm.fit(numeric_data)\n",
    "\n",
    "    # Predict outliers: +1 for inliers, -1 for outliers\n",
    "    outlier_classification_scores = algorithm.predict(numeric_data)\n",
    "    outlier_distance_scores = algorithm.score_samples(numeric_data)\n",
    "\n",
    "    # Add distance scores to the corresponding rows in the original DataFrame\n",
    "    df.loc[numeric_data.index, \"oneclass_svm_degree\"] = outlier_distance_scores\n",
    "\n",
    "    # Map outlier classification scores to a boolean column: True for outliers, False for inliers\n",
    "    outlier_series = pd.Series(False, index=df.index)\n",
    "    outlier_series[numeric_data.index] = (outlier_classification_scores == -1)\n",
    "    df[\"oneclass_svm_outlier\"] = outlier_series\n",
    "\n",
    "    # If kernel is linear, retrieve support vectors\n",
    "    support_vectors = numeric_data.iloc[algorithm.support_] if kernel == \"linear\" else None\n",
    "\n",
    "    # Plot sorted outlier degrees\n",
    "    sorted_outlier_degrees = sorted(outlier_distance_scores)\n",
    "    sns.lineplot(\n",
    "        x=range(len(sorted_outlier_degrees)),\n",
    "        y=sorted_outlier_degrees\n",
    "    )\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_outliers = one_class_svm(df_with_outliers)\n",
    "\n",
    "outlier_rows = df_with_outliers[df_with_outliers[\"oneclass_svm_outlier\"] == True]\n",
    "print(outlier_rows)\n",
    "\n",
    "# Drop the oneclass_svm_degree column since we don't want it from now on\n",
    "df_with_outliers.drop('oneclass_svm_degree', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Isolation forest](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_forest(df):\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "\n",
    "    # Select only numeric columns from df\n",
    "    numeric_columns = df.select_dtypes(include=\"number\")\n",
    "    numeric_data = numeric_columns.dropna()  # Drop rows with missing numeric values. Consider imputation if needed.\n",
    "\n",
    "    # Initialize the Isolation Forest model\n",
    "    max_features = numeric_data.shape[1]  # Use the number of features in the numeric data\n",
    "    algorithm = IsolationForest(max_features=max_features)\n",
    "    algorithm.fit(numeric_data)\n",
    "\n",
    "    # Calculate outlier degrees and scores\n",
    "    outlier_degrees = algorithm.decision_function(numeric_data) + 0.5\n",
    "    outlier_scores = 1 - (algorithm.predict(numeric_data) + 1) / 2  # 1 for outliers, 0 for inliers\n",
    "\n",
    "    # Assign degrees and scores to the corresponding rows in the original DataFrame\n",
    "    df.loc[numeric_data.index, \"isolation_forest_degree\"] = outlier_degrees\n",
    "    df.loc[numeric_data.index, \"isolation_forest_scores\"] = outlier_scores\n",
    "\n",
    "    # Create a boolean column for outlier identification\n",
    "    outlier_series = pd.Series(False, index=df.index)\n",
    "    outlier_series[numeric_data.index] = (outlier_scores == 1)\n",
    "    df[\"isolation_forest_outlier\"] = outlier_series\n",
    "\n",
    "    # Plot sorted outlier degrees\n",
    "    sorted_outlier_degrees = sorted(outlier_degrees)\n",
    "    sns.lineplot(\n",
    "        x=range(len(sorted_outlier_degrees)),\n",
    "        y=sorted_outlier_degrees\n",
    "    )\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_outliers = isolation_forest(df_with_outliers)\n",
    "\n",
    "outlier_rows = df_with_outliers[df_with_outliers[\"isolation_forest_outlier\"] == True]\n",
    "outlier_rows\n",
    "\n",
    "# Drop the oneclass_svm_degree column since we don't want it from now on\n",
    "df_with_outliers.drop('isolation_forest_degree', axis=1, inplace=True)\n",
    "df_with_outliers.drop('isolation_forest_scores', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[Get the final list of 'outlier' columns get getting the columns that were identified by a majority of tests](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for outliers\n",
    "outlier_columns = [\n",
    "    'isolation_forest_outlier', \n",
    "    'oneclass_svm_outlier', \n",
    "    'connectivity_approach_outlier', \n",
    "    'gmm_outlier', \n",
    "    'pca_outlier'\n",
    "]\n",
    "\n",
    "# Select rows where at least 3 out of the 5 specified columns are True (indicating outliers)\n",
    "df_outliers = df_with_outliers[df_with_outliers[outlier_columns].sum(axis=1) >= 3].drop(columns=outlier_columns)\n",
    "\n",
    "# Select rows where fewer than 3 out of the 5 specified columns are True (indicating non-outliers)\n",
    "df_non_outliers = df_with_outliers[df_with_outliers[outlier_columns].sum(axis=1) < 3].drop(columns=outlier_columns)\n",
    "\n",
    "# Write the DataFrames to CSV files\n",
    "df_outliers.to_csv('../dataset/df_races_only_outliers.csv', index=False)\n",
    "df_non_outliers.to_csv('../dataset/df_races_no_outliers.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
