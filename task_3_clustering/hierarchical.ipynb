{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'algoritmo\n",
    "\n",
    "Il clustering gerarchico è una tecnica di machine learning non supervisionato utilizzata per raggruppare dati in gruppi (cluster) basati sulla similarità tra loro. A differenza del clustering \"flat\" come il K-means, che suddivide i dati in un numero predefinito di cluster, il clustering gerarchico costruisce una struttura ad albero (dendrogramma) che rappresenta le relazioni gerarchiche tra i dati. Si divide in due approcci principali:\n",
    "\n",
    "1. Clustering agglomerativo: Parte trattando ciascun punto dati come un cluster individuale e li unisce gradualmente, in base alla loro similarità, fino a formare un unico cluster che include tutti i punti. È quindi un processo di bottom-up.\n",
    "2. Clustering divisivo: Parte da un unico cluster che contiene tutti i dati e divide progressivamente i cluster in gruppi più piccoli, fino ad arrivare ai singoli punti dati. Questo è un processo di top-down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Agglomerativo\n",
    "\n",
    "Nel clustering agglomerativo, vengono seguiti questi passaggi:\n",
    "\n",
    "1. Calcolare le distanze tra ogni coppia di punti (usando metriche come la distanza euclidea o quella di Manhattan).\n",
    "2. Unire i due punti o cluster più simili per creare un nuovo cluster.\n",
    "3. Aggiornare la matrice delle distanze, calcolando le distanze tra il nuovo cluster e gli altri cluster esistenti.\n",
    "4. Ripetere il processo fino a che non rimanga un solo cluster contenente tutti i dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metriche di similarità\n",
    "\n",
    "Le metriche di similarità sono fondamentali per il clustering gerarchico perché determinano il modo in cui vengono uniti i cluster durante il processo. Ogni metrica ha i suoi punti di forza e debolezze, quindi la scelta migliore dipende dal tipo di dati, dalla distribuzione e dall'obiettivo del clustering.\n",
    "\n",
    "1. Single Linkage (Distanza Minima)\n",
    "La distanza tra due cluster è definita come la distanza minima tra i punti dei due cluster. In altre parole, due cluster vengono uniti se il punto più vicino di un cluster è molto vicino a un punto dell'altro cluster.\n",
    "\n",
    "    Vantaggi:\n",
    "\n",
    "    - Conserva la connessione tra i dati: Single linkage è utile quando vogliamo che anche le connessioni più deboli (cioè i punti più vicini tra cluster) siano prese in considerazione.\n",
    "    - Adatto a dati di tipo connettivo: Può identificare cluster di forma irregolare o allungata, il che è utile quando i cluster sono distribuiti in modo non sferico.\n",
    "\n",
    "    Svantaggi:\n",
    "\n",
    "    - Fenomeno dell’“effetto chaining”: Tendendo a formare cluster allungati, può includere troppi punti in un cluster.\n",
    "    - Sensibile al rumore: I punti outlier o lontani possono distorcere la struttura del cluster.\n",
    "\n",
    "    Quando usarlo:\n",
    "\n",
    "    Se si desidera trovare cluster \"a catena\" o molto estesi, come nei dati di tipo geografico o nei grafi sociali, dove le connessioni sono fondamentali.\n",
    "\n",
    "2. Complete Linkage (Distanza Massima)\n",
    "La distanza tra due cluster è data dalla distanza massima tra i punti dei due cluster. Quindi, la fusione di due cluster è determinata dalla distanza tra i punti più distanti dei due cluster.\n",
    "\n",
    "    Vantaggi:\n",
    "\n",
    "    - Cluster più compatti e omogenei: I cluster risultanti tendono ad avere un’alta similarità interna, con meno estensioni verso l'esterno.\n",
    "    - Meno influenzato dall’effetto chaining: Rispetto al single linkage, tende a creare cluster più rotondi e compatti.\n",
    "\n",
    "    Svantaggi:\n",
    "\n",
    "    - Non adatto per cluster di forme allungate: Poiché si concentra sulla distanza massima, penalizza i cluster estesi, creando più cluster piccoli e compatti.\n",
    "    - Computazionalmente più complesso rispetto ad altre metriche.\n",
    "\n",
    "    Quando usarlo:\n",
    "\n",
    "    Quando si vogliono ottenere cluster ben separati e compatti, come nell’analisi dei segmenti di clienti o nelle analisi delle immagini.\n",
    "\n",
    "3. Average Linkage (Distanza Media)\n",
    "La distanza tra due cluster è data dalla media di tutte le distanze tra i punti dei due cluster. Questa metrica rappresenta un compromesso tra le distanze minime e massime.\n",
    "\n",
    "    Vantaggi:\n",
    "\n",
    "    - Compromesso bilanciato: Media i pro e i contro delle altre metriche, producendo cluster né troppo compatti né troppo estesi.\n",
    "    - Robusto rispetto a dati rumorosi: Non è influenzato dai singoli outlier come nel caso del single linkage.\n",
    "\n",
    "    Svantaggi:\n",
    "\n",
    "    - Calcoli intensivi: La necessità di calcolare tutte le distanze può essere computazionalmente impegnativo per dataset grandi.\n",
    "\n",
    "    Quando usarlo:\n",
    "\n",
    "    Quando si cerca un bilanciamento tra compattezza e flessibilità, come per dati complessi o distribuiti in modo variabile.\n",
    "\n",
    "4. Centroid Linkage (Distanza tra i Centroidi)\n",
    "In questo caso, la distanza tra due cluster è definita come la distanza tra i centroidi (media aritmetica dei punti) di ciascun cluster.\n",
    "\n",
    "    Vantaggi:\n",
    "\n",
    "    - Mantiene il centro del cluster come riferimento: I cluster sono modellati attorno ai loro centroidi, producendo una struttura più stabile e ben definita.\n",
    "    - Efficienza nei calcoli: Calcolare un singolo centroide è computazionalmente meno intenso.\n",
    "\n",
    "    Svantaggi:\n",
    "\n",
    "    - Rischio di distorsione: È sensibile a cluster di dimensioni o densità molto diverse.\n",
    "    - Non adatto a cluster non convessi: Tende a funzionare meglio quando i dati sono distribuiti in modo più simmetrico o rotondo.\n",
    "\n",
    "    Quando usarlo:\n",
    "\n",
    "    Nei dati con una distribuzione ben definita e simmetrica, come nei casi di cluster sferici o quando si conosce la presenza di centroidi ben definiti.\n",
    "\n",
    "5. Metodo di ward\n",
    "Strategia specifica per il clustering gerarchico agglomerativo, progettata per minimizzare la varianza all'interno dei cluster risultanti. La sua caratteristica distintiva è che, a ogni passo del processo di clustering, vengono uniti i cluster che causano il minor incremento possibile nella sommatoria delle deviazioni quadratiche (varianza) rispetto ai punti centrali dei cluster.\n",
    "\n",
    "    Nel clustering agglomerativo, ogni coppia di cluster viene valutata per vedere quale fusione causerebbe il minor aumento nella somma delle distanze quadratiche di ciascun punto dati dal centroide del cluster. In pratica:\n",
    "\n",
    "    1. Si calcolano le somme delle deviazioni quadratiche di ciascun punto dati rispetto al centroide per ogni coppia di cluster.\n",
    "\n",
    "    2. Si fonde la coppia di cluster che minimizza questa somma, così da mantenere la varianza interna al cluster il più bassa possibile.\n",
    "\n",
    "    A differenza di altri metodi di linkage (come il single, complete o average linkage), il metodo di Ward considera la varianza, rendendolo particolarmente efficace quando si cerca di creare cluster più omogenei. Per questo, i cluster risultanti tendono ad avere dimensioni più simili e distribuzioni più equilibrate.\n",
    "\n",
    "    Vantaggi:\n",
    "\n",
    "    - Omogeneità: Crea cluster con varianza interna bassa, quindi i punti all'interno di ogni cluster sono molto simili.\n",
    "    - Bilanciamento: Evita la formazione di cluster troppo sbilanciati o di dimensioni molto diverse.\n",
    "    - Adatto a dati continui: È particolarmente adatto per dati con variabili numeriche, in cui è importante la distanza euclidea o la similarità tra valori continui.\n",
    "\n",
    "    Svantaggi:\n",
    "\n",
    "    - Computazionalmente costoso: Richiede una grande quantità di calcoli di varianza, specialmente per grandi dataset.\n",
    "\n",
    "    - Sensibile ai valori anomali: Gli outlier possono influenzare negativamente la varianza interna dei cluster, portando a risultati meno accurati.\n",
    "\n",
    "    Il metodo di Ward è ampiamente utilizzato per applicazioni in cui è essenziale avere cluster compatti e ben distinti, come nelle analisi di segmentazione dei clienti, bioinformatica e analisi di mercato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features \"supportate\"\n",
    "\n",
    "\n",
    "Il clustering gerarchico è versatile e può essere applicato a diversi tipi di features (caratteristiche), come numeriche, categoriali, binarie e testuali. Tuttavia, richiede spesso una fase di preprocessing per garantire che le metriche di similarità utilizzate siano appropriate e che i risultati siano significativi.\n",
    "\n",
    "1. Features Numeriche\n",
    "Le features numeriche sono quelle più comunemente usate per il clustering gerarchico. Le metriche di distanza, come la distanza euclidea o quella di Manhattan, si applicano direttamente a questo tipo di dati.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Standardizzazione (Z-score scaling): Portare i dati numerici su una scala comune con media 0 e deviazione standard 1, utile se i dati hanno unità diverse o differenze di scala. La standardizzazione riduce il peso sproporzionato delle variabili con varianze maggiori.\n",
    "    - Normalizzazione (Min-Max scaling): Ridimensiona i dati in un intervallo, ad esempio [0, 1]. È utile se si desidera che ogni variabile numerica contribuisca equamente.\n",
    "    \n",
    "    Quando usare: Se i valori numerici rappresentano grandezze fisiche o variabili continue. Questo preprocessing è particolarmente utile per il metodo di Ward o per metriche di distanza come la distanza euclidea.\n",
    "\n",
    "2. Features Categoriali\n",
    "Le features categoriali rappresentano informazioni in formato qualitativo, come il colore o la tipologia di un oggetto. I valori sono in genere non numerici (ad esempio, \"rosso\", \"blu\", \"giallo\") e non hanno un ordine intrinseco.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Codifica One-Hot (dummy variables): Trasforma ogni categoria in una variabile binaria, rappresentando ogni livello con un valore di 0 o 1. Questo permette di utilizzare metriche di similarità che richiedono dati numerici.\n",
    "    - Codifica Ordinale: Assegna valori numerici ordinati ai livelli di una variabile categorica solo se c'è un ordine naturale tra le categorie (come le taglie: S, M, L, XL).\n",
    "    \n",
    "    Quando usare: Se i dati includono variabili categoriche e si desidera misurare la distanza tra punti anche in base a queste categorie. È utile per settori come il retail, dove variabili come la categoria del prodotto o la fascia d’età dei clienti possono influenzare i cluster.\n",
    "\n",
    "3. Features Binarie\n",
    "Le features binarie contengono solo due valori (ad esempio, 0 e 1, o vero e falso). Sono comuni nei dataset che rappresentano la presenza o assenza di certe caratteristiche.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Codifica binaria: Normalmente, i dati binari non richiedono ulteriori preprocessamenti poiché 0 e 1 sono già interpretabili. Tuttavia, è importante considerare l'interpretazione dei valori binari.\n",
    "    - Metriche di similarità specifiche: Per dati binari, possono essere usate metriche specifiche, come il coefficiente di Jaccard o la distanza di Hamming. Queste metriche funzionano direttamente sui dati binari senza trasformazioni aggiuntive.\n",
    "    \n",
    "    Quando usare: È adatto per dati come le risposte a questionari (ad esempio, \"sì\" o \"no\") e in dataset di presenza/assenza di caratteristiche (ad esempio, sintomi di una malattia). Queste metriche di similarità possono aiutare a trovare gruppi di utenti o campioni simili.\n",
    "\n",
    "4. Features Testuali\n",
    "Le features testuali sono non strutturate e richiedono un trattamento particolare per trasformare i dati in una forma numerica.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Bag-of-Words (BOW): Rappresenta ogni documento come un vettore di conteggi delle parole. Questa trasformazione produce vettori numerici che rappresentano la frequenza delle parole.\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF): Trasforma il testo in rappresentazioni numeriche ponderando le parole in base alla loro frequenza e alla rilevanza. La TF-IDF è utile per ridurre il peso delle parole comuni e aumentare l’importanza di parole chiave.\n",
    "    - Word Embeddings (come Word2Vec o BERT): Forniscono rappresentazioni dense e continue delle parole o delle frasi in uno spazio numerico. Questi vettori sono spesso più efficaci per catturare il significato semantico.\n",
    "    \n",
    "    Quando usare: Se il dataset è basato su testo, come recensioni, descrizioni di prodotti o articoli. Con metriche di similarità come la distanza coseno, il clustering può rivelare gruppi di documenti o frasi con contenuti simili.\n",
    "\n",
    "5. Features Temporali\n",
    "Le features temporali (come data e ora) possono essere trattate in vari modi, a seconda del tipo di analisi che si desidera fare.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Trasformazioni di data e ora: Le variabili di data e ora possono essere decomposte in componenti significative, come l'ora del giorno, il giorno della settimana o il mese, a seconda della granularità temporale desiderata.\n",
    "    - Differenze temporali: Calcolare differenze di tempo tra eventi (ad esempio, giorni trascorsi) può essere utile per trasformare le variabili temporali in rappresentazioni numeriche.\n",
    "    - Fourier Transform: Se si tratta di dati ciclici o con pattern periodici, la decomposizione tramite Fourier può aiutare a identificare i pattern ricorrenti.\n",
    "    \n",
    "    Quando usare: Se si vuole scoprire pattern o cluster basati su variazioni temporali (ad esempio, abitudini di acquisto nel corso della settimana). Tali dati sono comuni nei dati di accesso a siti web, nelle vendite di e-commerce e nei dati di traffico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering agglomerativo in Scikit-Learn\n",
    "\n",
    "L'algoritmo AgglomerativeClustering in scikit-learn offre una serie di parametri configurabili che consentono di personalizzare il processo di clustering gerarchico in base alle caratteristiche del dataset e agli obiettivi dell’analisi. Ecco una panoramica completa dei parametri disponibili:\n",
    "\n",
    "1. n_clusters\n",
    "- Descrizione: Numero di cluster finali desiderato.\n",
    "- Tipo: int, opzionale.\n",
    "- Valore di default: 2.\n",
    "- Note: Se n_clusters è impostato, il clustering continuerà fino a ottenere il numero specificato di cluster. Se invece viene impostato distance_threshold, questo parametro viene ignorato.\n",
    "\n",
    "2. distance_threshold\n",
    "- Descrizione: Distanza massima per fermare il processo di fusione dei cluster.\n",
    "- Tipo: float, opzionale.\n",
    "- Valore di default: None.\n",
    "- Note: Definisce la soglia a cui viene tagliato il dendrogramma per determinare il numero di cluster. Se impostato, n_clusters deve essere None. È utile se si desidera che il numero di cluster sia determinato in base a una distanza di soglia.\n",
    "\n",
    "3. affinity\n",
    "- Descrizione: Metrica di distanza per calcolare le somiglianze tra i punti.\n",
    "- Tipo: str, opzioni: 'euclidean', 'l1', 'l2', 'manhattan', 'cosine', oppure una funzione callable.\n",
    "- Valore di default: 'euclidean'.\n",
    "- Note: La scelta di affinity determina la metrica con cui viene calcolata la distanza tra i punti durante la fusione dei cluster. Se si usa linkage='ward', l’unica opzione disponibile è 'euclidean'.\n",
    "\n",
    "4. linkage\n",
    "- Descrizione: Metodo di linkage utilizzato per calcolare la distanza tra cluster.\n",
    "- Tipo: str, opzioni: 'ward', 'complete', 'average', 'single'.\n",
    "- Valore di default: 'ward'.\n",
    "- Note:\n",
    "    - 'ward': Minimizza la varianza interna dei cluster (richiede affinity='euclidean').\n",
    "    - 'complete': Utilizza la distanza massima tra i punti dei cluster (produce cluster più compatti).\n",
    "    - 'average': Usa la distanza media tra i punti di due cluster.\n",
    "    - 'single': Usa la distanza minima tra i punti di due cluster (tende a formare cluster allungati).\n",
    "\n",
    "5. compute_full_tree\n",
    "- Descrizione: Indica se calcolare o meno l'intero albero di clustering.\n",
    "- Tipo: bool o 'auto'.\n",
    "- Valore di default: 'auto'.\n",
    "- Note:\n",
    "    - Se n_clusters è piccolo rispetto al numero totale di campioni, è possibile impostarlo su False per aumentare la velocità di calcolo.\n",
    "    'auto': Disattiva il calcolo dell’intero albero solo se n_clusters è inferiore a una certa soglia.\n",
    "    - Impostare su True se si vuole ottenere il dendrogramma completo per visualizzare l'intera gerarchia.\n",
    "\n",
    "6. connectivity\n",
    "- Descrizione: Matrice di connettività per definire le connessioni tra punti.\n",
    "- Tipo: array, sparse matrix o None.\n",
    "- Valore di default: None.\n",
    "- Note: La matrice di connettività limita il clustering a specifiche connessioni tra punti, utilissima per dati strutturati spazialmente (es. griglie o grafi). In pratica, indica quali punti possono essere uniti direttamente.\n",
    "\n",
    "7. compute_distances\n",
    "- Descrizione: Se impostato su True, memorizza le distanze tra cluster durante le fusioni.\n",
    "- Tipo: bool, opzionale.\n",
    "- Valore di default: False.\n",
    "- Note: Utile per visualizzare o analizzare le distanze di linkage nel dendrogramma. Se True, i valori di distanza saranno accessibili nell'attributo distances_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering divisivo\n",
    "\n",
    "Il processo del clustering divisivo può essere riassunto in questi passaggi:\n",
    "\n",
    "1. Inizializzazione: Si parte da un cluster che contiene tutti i dati.\n",
    "2. Suddivisione iterativa: Si sceglie il cluster da suddividere in due sottogruppi, selezionando il criterio che massimizza la separazione tra i nuovi cluster. Questo può essere fatto utilizzando algoritmi di partizione come il K-means (impostato per creare due cluster) o altre tecniche di suddivisione.\n",
    "3. Ripetizione del processo: Ogni sottocluster viene successivamente diviso in cluster più piccoli, seguendo lo stesso criterio, fino a che non si raggiunge il livello di granularità desiderato o fino a che ogni cluster contenga un singolo punto.\n",
    "\n",
    "\n",
    "Caratteristiche del Clustering Gerarchico Divisivo\n",
    "- Struttura gerarchica: Il risultato è una struttura ad albero, chiamata dendrogramma, che mostra come i cluster sono stati divisi nel tempo.\n",
    "- Scelta della divisione: Ogni divisione è fatta in modo da massimizzare la dissimilarità tra i cluster generati. Si cerca di ottenere cluster ben distinti, spesso usando tecniche che minimizzano la varianza interna o massimizzano la distanza tra i cluster.\n",
    "- Computazionalmente costoso: Rispetto al clustering agglomerativo, il clustering divisivo è spesso più impegnativo in termini di calcoli, perché richiede il ricalcolo delle divisioni ottimali a ogni passaggio.\n",
    "\n",
    "Approcci per la Divisione\n",
    "\n",
    "Diversi approcci possono essere utilizzati per dividere i cluster:\n",
    "\n",
    "- K-means (con K=2): Per dividere un cluster in due, è comune applicare il K-means per trovare i due sottogruppi che massimizzano la differenza. Tuttavia, l'uso del K-means può essere costoso per grandi cluster.\n",
    "- Massimizzazione della distanza: Se i dati sono rappresentati in uno spazio di distanza, si possono identificare due punti lontani e costruire cluster attorno a essi.\n",
    "- Minimizzazione della varianza interna: Simile al metodo di Ward, si cerca di creare cluster in modo che la varianza dei punti all'interno di ogni sottocluster sia minima.\n",
    "\n",
    "Vantaggi del Clustering Divisivo\n",
    "\n",
    "- Maggiore controllo sulle divisioni: Si può scegliere a ogni passaggio quale cluster dividere, rendendo possibile una segmentazione più fine dei dati.\n",
    "- Ben definiti quando i cluster principali sono pochi: Se i dati contengono pochi cluster principali con sottostrutture interne, il clustering divisivo può essere più preciso rispetto all’approccio agglomerativo.\n",
    "\n",
    "Svantaggi del Clustering Divisivo\n",
    "- Computazionalmente intensivo: Il clustering divisivo è spesso più costoso rispetto all'agglomerativo, poiché a ogni passo si deve calcolare la divisione ottimale.\n",
    "- Suscettibile a errori iniziali: Le prime divisioni influenzano notevolmente i cluster finali. Se la prima suddivisione non è buona, i risultati possono risentirne.\n",
    "- Meno usato rispetto all'agglomerativo: Data la complessità computazionale e la difficoltà di implementazione, il clustering divisivo è meno utilizzato rispetto all’approccio agglomerativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features \"supportate\"\n",
    "\n",
    "1. Features Numeriche\n",
    "Per il clustering divisivo, le features numeriche sono ideali poiché le metriche di distanza (come la distanza euclidea) e gli algoritmi di partizione (come il K-means) si applicano direttamente.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Standardizzazione (Z-score scaling): Come nel clustering agglomerativo, è consigliata la standardizzazione per portare tutte le variabili su una scala comune, soprattutto se si utilizza il K-means per dividere i cluster.\n",
    "    - Normalizzazione (Min-Max scaling): È utile se si vuole che le variabili numeriche abbiano un contributo equo, evitando che variabili con range più grandi dominino la suddivisione.\n",
    "    Considerazioni aggiuntive:\n",
    "\n",
    "    Nel clustering divisivo, le prime divisioni sono cruciali, quindi una buona standardizzazione è particolarmente importante per evitare cluster iniziali squilibrati che influenzerebbero la qualità delle suddivisioni successive.\n",
    "\n",
    "2. Features Categoriali\n",
    "Le features categoriali possono essere gestite anche nel clustering divisivo, ma richiedono una trasformazione in formato numerico per permettere il calcolo delle distanze.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Codifica One-Hot: Rappresenta ogni livello della variabile categoriale come una variabile binaria separata. La codifica One-Hot è utile per applicare tecniche di partizione come il K-means, trasformando ogni categoria in un valore numerico.\n",
    "    - Codifica Ordinale: Usare questa codifica solo per categorie con un ordine naturale, in modo da rispettare le distanze tra le categorie.\n",
    "    Considerazioni aggiuntive:\n",
    "\n",
    "    Se si utilizza una tecnica di partizione come il K-means per le divisioni, bisogna considerare che la codifica categoriale può portare a cluster sbilanciati, a meno che le categorie non siano ben bilanciate nel dataset.\n",
    "\n",
    "3. Features Binarie\n",
    "Le features binarie, già in formato numerico, sono adatte sia al clustering agglomerativo che a quello divisivo, soprattutto se si utilizzano metriche di distanza specifiche per dati binari.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Codifica binaria: Non richiede trasformazioni aggiuntive poiché 0 e 1 sono già interpretabili.\n",
    "    - Metriche di similarità specifiche: Utilizzare metriche come la distanza di Hamming o il coefficiente di Jaccard per calcolare la similarità tra cluster binari.\n",
    "    \n",
    "    Considerazioni aggiuntive:\n",
    "\n",
    "    Nel clustering divisivo, l’uso di metriche specifiche come la distanza di Hamming può aiutare a garantire che le prime divisioni siano ben rappresentative della struttura interna dei dati.\n",
    "\n",
    "4. Features Testuali\n",
    "Per le features testuali, è necessario trasformare i dati in vettori numerici per calcolare le distanze tra i punti e dividere correttamente i cluster.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Bag-of-Words (BOW): Convertire i testi in vettori di conteggi delle parole, permettendo al clustering di suddividere i dati in base alla presenza o assenza di parole comuni.\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF): Ponderare le parole per frequenza e rilevanza, utile per differenziare i cluster in base alle parole chiave.\n",
    "    - Word Embeddings: Per un clustering più accurato, gli embeddings (come Word2Vec o BERT) sono ideali per rappresentare significati semantici, facilitando la divisione dei cluster.\n",
    "    \n",
    "    Considerazioni aggiuntive:\n",
    "\n",
    "    La scelta tra BOW, TF-IDF o embeddings dipende dalla granularità delle divisioni desiderate. Gli embeddings, ad esempio, possono fornire cluster semantici ben definiti nelle prime suddivisioni.\n",
    "\n",
    "5. Features Temporali\n",
    "    Le features temporali, come data e ora, possono essere trasformate in variabili numeriche o cicliche prima del clustering divisivo, esattamente come per il clustering agglomerativo.\n",
    "\n",
    "    Preprocessing consigliato:\n",
    "\n",
    "    - Trasformazioni di data e ora: Scomporre i dati temporali in componenti significative (es. giorno della settimana, ora del giorno) o calcolare differenze temporali (es. giorni tra eventi).\n",
    "    - Fourier Transform: Utile per identificare periodicità e pattern ciclici, che potrebbero essere rilevanti nelle divisioni iniziali.\n",
    "    \n",
    "    Considerazioni aggiuntive:\n",
    "\n",
    "    Per il clustering divisivo, è importante scegliere attentamente le caratteristiche temporali più rilevanti, in modo che le prime divisioni rispecchino accuratamente il comportamento temporale del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differenze nel Preprocessing tra Clustering Divisivo e Agglomerativo\n",
    "Il preprocessing per il clustering divisivo è in gran parte simile a quello agglomerativo, ma ci sono alcune differenze:\n",
    "\n",
    "- Importanza delle prime divisioni: Nel clustering divisivo, le prime divisioni influenzano in modo significativo la struttura finale, quindi è essenziale un preprocessing che enfatizzi le caratteristiche più rilevanti, garantendo una rappresentazione coerente.\n",
    "- Bilanciamento delle features: Poiché ogni divisione successiva si basa su quella precedente, è importante bilanciare le features in modo che ogni variabile contribuisca adeguatamente, riducendo il rischio che le prime divisioni siano dominate da variabili con grande varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretazione tramite Dendrogramma\n",
    "Il risultato del clustering gerarchico viene rappresentato tramite un dendrogramma, un grafico ad albero che visualizza i livelli di fusione dei cluster. Ogni biforcazione nel dendrogramma rappresenta una fusione, e l’altezza della biforcazione indica la distanza o dissimilarità tra i cluster uniti.\n",
    "\n",
    "### Vantaggi e Svantaggi\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "- Non richiede di specificare il numero di cluster in anticipo.\n",
    "- Crea una struttura gerarchica, utile per visualizzare relazioni multi-livello tra dati.\n",
    "\n",
    "Svantaggi:\n",
    "\n",
    "Computazionalmente intenso per grandi dataset.\n",
    "- Sensibile alle scelte delle metriche di similarità e linkage, che possono influenzare i risultati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### MST\n",
    "\n",
    "Nel clustering divisivo, l'albero di copertura minimo (MST) può essere utilizzato per facilitare la divisione dei dati in cluster in modo efficiente, sfruttando la struttura del grafo dei punti dati. Il MST aiuta a identificare i collegamenti essenziali tra i punti che riducono la complessità della struttura a connessioni essenziali senza cicli, consentendo di individuare le aree di maggiore separazione.\n",
    "\n",
    "Come Funziona l’MST nel Clustering Divisivo\n",
    "\n",
    "1. Costruzione del Grafo: Si rappresentano i punti dati come nodi di un grafo e si calcolano le distanze tra ciascuna coppia di punti. Questo grafo è ponderato, poiché a ciascun arco viene assegnato un peso corrispondente alla distanza tra i punti connessi.\n",
    "\n",
    "2. Calcolo dell'MST: Si applica un algoritmo di Minimum Spanning Tree (come Kruskal o Prim) per trovare un albero di copertura minimo. L'MST copre tutti i nodi del grafo usando il minor numero di archi e minimizzando il peso complessivo degli archi, formando così una struttura senza cicli.\n",
    "\n",
    "3. Identificazione delle Divisioni: Per suddividere i cluster, si individuano gli archi più “deboli” (quelli con peso maggiore) nell'MST e si rimuovono. Questo taglio divide il grafo in sottoalberi, che rappresentano cluster separati. Gli archi di peso maggiore solitamente indicano separazioni naturali nei dati, quindi il taglio di questi archi tende a creare cluster più distinti.\n",
    "\n",
    "4. Ripetizione del Processo: Ogni sottoalbero (cluster) può essere ulteriormente diviso applicando lo stesso processo, cioè calcolando l'MST per i punti rimanenti nel sottocluster e tagliando gli archi di maggiore peso fino a ottenere la granularità desiderata.\n",
    "\n",
    "Vantaggi dell’Uso dell’MST nel Clustering Divisivo:\n",
    "\n",
    "- Identificazione naturale delle separazioni: Gli archi di maggiore lunghezza nel MST spesso indicano le maggiori separazioni tra gruppi di punti, rendendo il taglio dell’MST un approccio naturale per suddividere i cluster.\n",
    "- Riduzione della complessità: L’MST rappresenta solo le connessioni essenziali, semplificando il grafo e rendendo il calcolo delle divisioni più efficiente.\n",
    "- Adattabile a cluster di forma irregolare: Poiché non si basa su centroidi (come il K-means), l'MST è utile per identificare cluster di forma non sferica o con strutture complesse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues nel clustering gerarchico\n",
    "\n",
    "1. Scelta della Metodologia: Agglomerativo vs. Divisivo\n",
    "\n",
    "- Agglomerativo: Inizia unendo punti singoli, e quindi gli errori nelle fusioni iniziali possono propagarsi, influenzando negativamente la qualità dei cluster finali. Inoltre, il clustering agglomerativo può essere computazionalmente meno oneroso del divisivo, ma comunque intensivo per dataset molto grandi.\n",
    "- Divisivo: Parte da un singolo cluster e divide progressivamente, con il rischio che le prime divisioni determinino una struttura meno ottimale. È meno usato rispetto all’agglomerativo, principalmente a causa dell’elevato costo computazionale, quindi è più adatto a dataset di dimensioni contenute o altamente strutturati.\n",
    "\n",
    "2. Scelta della Metodica di Linkage\n",
    "\n",
    "La metrica di linkage influenza fortemente la forma e la struttura dei cluster:\n",
    "\n",
    "- Single Linkage (distanza minima): Tende a creare cluster allungati, adatti a cluster di forma irregolare. Tuttavia, soffre dell’effetto chaining, in cui i punti vicini possono \"allungare\" eccessivamente un cluster.\n",
    "- Complete Linkage (distanza massima): Crea cluster più compatti, ma può non rilevare correttamente cluster allungati o di forma irregolare.\n",
    "- Average Linkage (distanza media): Offre un compromesso tra complete e single linkage, ma può essere computazionalmente più oneroso.\n",
    "- Ward: Minimizza la varianza interna e produce cluster compatti, ma può richiedere elevati calcoli e non gestisce bene cluster non sferici o di dimensioni molto diverse.\n",
    "\n",
    "3. Scelta della Metrica di Distanza\n",
    "- Distanza Euclidea: È la metrica più comune per dati numerici, ma è sensibile alla scala dei dati. Le variabili con varianza maggiore possono influenzare eccessivamente i risultati.\n",
    "- Distanza di Manhattan: Migliore per dati con componenti sparse o distribuzioni rettilinee.\n",
    "- Metriche specializzate: Ad esempio, la distanza di Hamming o il coefficiente di Jaccard, per dati binari o categorici.\n",
    "\n",
    "    È fondamentale scegliere la metrica di distanza appropriata in base alla natura dei dati e assicurarsi di standardizzarli quando le unità di misura differiscono o quando una variabile ha un range molto più ampio rispetto alle altre.\n",
    "\n",
    "4. Preprocessing dei Dati\n",
    "- Standardizzazione: Per le variabili numeriche, la standardizzazione o normalizzazione è essenziale se le variabili hanno scale diverse. La mancata standardizzazione può causare la predominanza di una variabile sui risultati.\n",
    "- Codifica delle variabili categoriche: Le variabili categoriche devono essere trasformate in valori numerici utilizzabili nelle metriche di distanza. La codifica One-Hot è comune per il clustering gerarchico.\n",
    "- Rimozione di Outlier: Gli outlier possono distorcere i cluster, soprattutto con metodi di linkage come single o complete. La rimozione preventiva o il trattamento di questi valori è cruciale per risultati più accurati.\n",
    "\n",
    "5. Sensibilità alla Dimensione del Dataset\n",
    "Il clustering gerarchico è computazionalmente intensivo, con una complessità quadratica rispetto al numero di punti dati O(n^2). Per dataset di grandi dimensioni, ciò può essere proibitivo:\n",
    "\n",
    "- Ottimizzazione: Per dataset di grandi dimensioni, è possibile utilizzare campionamenti, calcolare l’MST come pre-processing o considerare metodi alternativi per ridurre i calcoli.\n",
    "- Algoritmi approssimati: Alcune versioni approssimate del clustering gerarchico sono state sviluppate per lavorare su grandi dataset, ma potrebbero non produrre la stessa qualità di clustering.\n",
    "\n",
    "6. Scelta del Cut-Off del Dendrogramma\n",
    "\n",
    "- Il dendrogramma rappresenta visivamente la gerarchia dei cluster, ma determinare dove \"tagliare\" il dendrogramma per definire i cluster finali non è sempre ovvio. Un cut-off troppo alto riduce il numero di cluster, mentre un cut-off troppo basso li moltiplica eccessivamente.\n",
    "- Visualizzazione del dendrogramma: Un metodo comune è visualizzare il dendrogramma e cercare \"salti\" significativi nell’altezza, che indicano possibili separazioni naturali nei dati.\n",
    "- Calcolo di indici di qualità del clustering: Come l’indice di Silhouette, può aiutare a valutare la qualità del clustering in relazione al numero di cluster, sebbene il dendrogramma stesso rimanga uno strumento visivo primario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Validity\n",
    "\n",
    "Vedi k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='../dataset/merged_FE_no_outliers.csv')\n",
    "\n",
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns = ['BMI','race_difficulty','team_avg_strength','consistency_score', 'PWR', 'climb_efficiency','prestige_weighted_delta']\n",
    "df_reduced = df[columns]\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_reduced.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# Applicazione della PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calcoliamo la varianza spiegata per ciascuna componente principale\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Mostriamo la varianza spiegata cumulativa\n",
    "explained_variance_cumulative = explained_variance_ratio.cumsum()\n",
    "\n",
    "# Creazione di un DataFrame per mostrare i risultati\n",
    "pca_summary = pd.DataFrame({\n",
    "    'Component': [f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
    "    'Explained Variance Ratio': explained_variance_ratio,\n",
    "    'Cumulative Explained Variance': explained_variance_cumulative\n",
    "})\n",
    "\n",
    "pca_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) \n",
    "pca_result = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_selected_components = pca_result[:30000, -3:-1]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram = sch.dendrogram(sch.linkage(pca_selected_components, method='ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "df_reduced['hierarchical_cluster_ward'] = hierarchical_clustering.fit_predict(pca_selected_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram_complete = sch.dendrogram(sch.linkage(pca_selected_components, method='complete'))\n",
    "plt.title('Dendrogram (Complete Linkage)')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "df_reduced['hierarchical_cluster_complete'] = hierarchical_clustering.fit_predict(pca_selected_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram_average = sch.dendrogram(sch.linkage(pca_selected_components, method='average'))\n",
    "plt.title('Dendrogram (Average Linkage)')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "df_reduced['hierarchical_cluster_average'] = hierarchical_clustering.fit_predict(pca_selected_components)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
